{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcfb634",
   "metadata": {},
   "source": [
    "cart决策树包括cart分类树和cart回归树，基于前面的\"1_cart分类树\"和\"cart回归树\"算法可见，\n",
    "二者处理的不同在于计算方式是gini/variance。因此可以将二者统一成一个算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d9436",
   "metadata": {},
   "source": [
    "**cart分类树**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5b513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"通过树结点的各属性记录生成的树结构\"\"\"\n",
    "    def __init__(self,\n",
    "                 best_feature_i=None, \n",
    "                 best_split_point=None,\n",
    "                 left_node=None, \n",
    "                 right_node=None,\n",
    "                 leaf_class = None,\n",
    "                 is_leaf=False,\n",
    "                 gini=None):\n",
    "        \"\"\"\n",
    "        每个当前结点Node都记录了当前的划分状况\n",
    "        :param left_child_node : 结点的左侧子结点\n",
    "        :param right_child_node : 结点的右侧子结点\n",
    "        :param best_feature_i : 当前结点的最佳划分特征\n",
    "        :param split_point : 当前结点的最佳特征对应的最佳分割点\n",
    "        :param leaf_class : 记录当前节点所属的类别\n",
    "        :param is_leaf : 只有在is_leaf==True时，leaf_class才生效\n",
    "        :param gini : 当前节点的gini_index\n",
    "        \n",
    "        \"\"\"\n",
    "        self.best_feature_i = best_feature_i\n",
    "        self.best_split_point = best_split_point\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.leaf_class = leaf_class\n",
    "        self.is_leaf = is_leaf\n",
    "        self.gini = gini\n",
    "class CartDecisionTree():\n",
    "    \"\"\"使用cart算法构建决策树\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth = float(\"inf\"),min_sample_split=2, min_gini_decrease=None):\n",
    "        # 代表决策树的决策树根节点\n",
    "        self.root_node = None \n",
    "        # 预设的决策树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 预设的决策树叶子节点最小样本数\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # 预设的基尼系数增益的最小值（gini_gain太小时不划分）\n",
    "        self.min_gini_decrease = min_gini_decrease\n",
    "    def fit(self, X,y,is_linear=False):\n",
    "        \"\"\"\n",
    "        决策树拟合\n",
    "        :param X : 训练数据集∈（m,n）\n",
    "        :param y : 训练标签∈（n,1）\n",
    "        :param is_linear : 特征是否为连续型\n",
    "        \n",
    "        \"\"\"\n",
    "        # 创建决策树根结点\n",
    "        self.root_node = Node()\n",
    "        # 默认根节点的深度为1\n",
    "        cur_depth = 1\n",
    "        # 根节点的初始化权重\n",
    "        # 样本的初始权重:都为1\n",
    "        weight = np.ones((len(X))) # 全局的weight:初始化为全1 \n",
    "        # 递归构建决策树\n",
    "        self._build_tree_recussive(X,y,np.arange(len(X)),weight,self.root_node, cur_depth, is_linear)\n",
    "    \n",
    "    def _build_tree_recussive(self, X,y, node_indices,weight,node:Node, cur_depth, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,递归建立决策树\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param node : 当前结点的状态记录\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = len(node_indices)\n",
    "        n_features = X.shape[1]\n",
    "        # 记录本节点的状态\n",
    "        node.gini = self._gini(y[node_indices], weight[node_indices])\n",
    "        node.leaf_class = self._majority_vote(y[node_indices], weight[node_indices])\n",
    "\n",
    "        ## 递归基\n",
    "        # 节点包含数据属于同一个类别，此时无需划分\n",
    "        if len(np.unique(y[node_indices])) <= 1:\n",
    "            # 记录叶子结点所属的分类\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 没有更多特征(当前节点所含样本所有特征都只有一个取值)\n",
    "        if np.sum([len(np.unique(X[node_indices][:,i])) for i in range(n_features)]) == n_features:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制构建子树的深度\n",
    "        if cur_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制节点的最小样本量\n",
    "        if n_samples < self.min_sample_split:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "        ## 处理当前节点自身\n",
    "        # 找到最佳特征和特征的最佳分割点\n",
    "        best_feature_i,best_gini_gain, best_sets = self._get_best_split_feature(X, y, node_indices, weight, is_linear)\n",
    "        \n",
    "        # 基尼系数增益的最小值（gini_gain太小时不划分）\n",
    "        if self.min_gini_decrease is not None and  best_gini_gain < self.min_gini_decrease:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        # 基于最佳特征和最佳分割点分成左右子树left,right\n",
    "        left_indices = best_sets[\"left_indices\"]\n",
    "        right_indices = best_sets[\"right_indices\"]\n",
    "        left_weight = best_sets[\"left_weight\"]\n",
    "        right_weight = best_sets[\"right_weight\"]\n",
    "        # 记录本节点的状态\n",
    "        node.best_feature_i = best_feature_i\n",
    "        node.best_split_point = best_sets[\"best_split_point\"]\n",
    "        node.left_node = Node()\n",
    "        node.right_node = Node()\n",
    "        # --leaf_class和gini在递归基时记录\n",
    "\n",
    "        # 让buildtree()帮忙划分左右子树\n",
    "        self._build_tree_recussive(X,y,left_indices,left_weight, node.left_node, cur_depth+1, is_linear)\n",
    "        self._build_tree_recussive(X,y,right_indices,right_weight, node.right_node, cur_depth+1, is_linear)\n",
    "                                   \n",
    "    def _get_best_split_feature(self,X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_gini_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的基尼增益以及分割的左右子树\n",
    "            cur_gini_gain,cur_branch_sets = self._get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_gini_gain = lou * cur_gini_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_gini_gain >= best_gini_gain:\n",
    "                best_gini_gain = cur_gini_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"], best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_gini_gain, best_sets\n",
    "    \n",
    "    def _get_best_split_point(self,X,y,node_indices, weight, feature_i, is_linear=False):\n",
    "\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "        :param weight: 所有样本的权重\n",
    "        :param is_linear : 特征的类型\n",
    "        :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "        \"\"\"\n",
    "        ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "        # 当前特征i的所有候选分割点\n",
    "        split_points, split_func = self._create_split_points(X[node_indices], feature_i, is_linear)\n",
    "\n",
    "        ## 产生最佳切分点\n",
    "        # 初始化\n",
    "        best_gini_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "        best_sets = None# 存储最佳切分点切分的左右分支\n",
    "        # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "        for point in split_points:\n",
    "            # 使用每个候选分割点进行二分分割\n",
    "            left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "            right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "            # 分好左右分支后计算划分后的gini_gain\n",
    "            # 左右分支的权重计算不再使用频数\n",
    "            w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "\n",
    "            # 未划分前的gini:使用该属性上无缺失的样本来计算\n",
    "            cur_gini = self._gini(y[node_indices], weight[node_indices])\n",
    "            # 未划分前-划分后==gini_gain\n",
    "            cur_gini_gain = cur_gini - (w_left*self._gini(y[left_indices],weight[left_indices]) + w_right*self._gini(y[right_indices], weight[right_indices]))\n",
    "\n",
    "            # 选择最佳的split point\n",
    "            if cur_gini_gain >= best_gini_gain:\n",
    "                best_gini_gain = cur_gini_gain\n",
    "                # 划分时传给左右子集的weight、indices均不同weigh\n",
    "                # weight在之后再重置\n",
    "                best_sets = {\n",
    "                    \"best_split_point\":point,\n",
    "                    \"left_indices\": left_indices,\n",
    "                    \"right_indices\": right_indices,\n",
    "                }\n",
    "        return best_gini_gain, best_sets\n",
    "    \n",
    "    def _create_split_points(self, X, feature_i, is_linear=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func\n",
    "    \n",
    "     \n",
    "    \n",
    "    def _gini(self,y, weight):\n",
    "        \n",
    "        \"\"\"\n",
    "        计算当前样本集合的gini指数\n",
    "        :param y : 当前集合对应的标签\n",
    "        :param weight : 当前样本集合对应的权重\n",
    "\n",
    "        \"\"\"\n",
    "        # 找出y中包含的所有类别\n",
    "        unique_y = np.unique(y)\n",
    "        weight_sum = np.sum(weight)\n",
    "        # 1.计算p_k:计算无缺失样本中第k类所占的比例\n",
    "        p_k = [np.sum(weight[y==k])/ weight_sum for k in unique_y ]\n",
    "        # 2.计算gini\n",
    "        gini = np.sum([p * (1-p) for p in p_k])\n",
    "        return gini\n",
    "    \n",
    "    def _majority_vote(self, y,weight):\n",
    "        \"\"\"\n",
    "        根据多数原则定叶子节点所属的分类\n",
    "        :param y : 当前集合对应的标签\n",
    "        :param weight : 前集合对应的权重\n",
    "        :return most_common_class叶子节点所属的分类\n",
    "        \"\"\"\n",
    "        most_common_class = None\n",
    "        max_distribution = 0\n",
    "        for k in np.unique(y): \n",
    "            distribution = np.sum(weight[y==k])\n",
    "            if distribution >= max_distribution:\n",
    "                max_distribution = distribution\n",
    "                most_common_class = k\n",
    "        return most_common_class\n",
    "    \n",
    "    def prune(self, alpha=0):\n",
    "        \"\"\"对决策树进行后剪枝\"\"\"\n",
    "        return self._pruning_node(self.root_node, alpha)\n",
    "        \n",
    "    def _pruning_node(self, node,alpha):\n",
    "        \"\"\"\n",
    "        :param node : 当前处理的节点\n",
    "        :param alpha: loss的参数，alpha≥0\n",
    "\n",
    "        \"\"\"\n",
    "        ## 递归基:当前节点是叶子节点则直接返回\n",
    "        if node.is_leaf:\n",
    "            return \n",
    "        ## 让递归函数帮忙处理左右子树\n",
    "        self._pruning_node(node.left_node, alpha)\n",
    "        self._pruning_node(node.right_node, alpha)\n",
    "\n",
    "        ## 处理当前节点\n",
    "        # 剪枝后\n",
    "        post_loss = node.gini + alpha * 1 \n",
    "        # 剪枝前\n",
    "        pre_loss = node.left_node.gini + node.right_node.gini + alpha * 2\n",
    "        # 比较剪枝前的loss与剪枝后的loss\n",
    "        if post_loss < pre_loss: # 剪枝后loss更小则剪枝（收回左右结点）\n",
    "            node.left_node = None\n",
    "            node.right_node = None\n",
    "            node.best_feature_i = None\n",
    "            node.best_split_point = None\n",
    "            node.is_leaf = True\n",
    "    \n",
    "    def predict(self,X,is_linear=False):\n",
    "        \"\"\"\n",
    "        :param X: 待预测的m个样本\n",
    "\n",
    "        \"\"\"\n",
    "        # 每一个样本都通过二叉搜索决策树树查找所属类别,决策树由其根节点作为代表\n",
    "        y_pred = [self._search_class(x, self.root_node, is_linear) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _search_class(self, x, node:Node, is_linear=False):\n",
    "        \"\"\"\n",
    "        : param x: 待预测所属分类的样本\n",
    "        : param node : 当前所在节点\n",
    "\n",
    "        \"\"\"\n",
    "        # 递归基\n",
    "        if node.is_leaf:# 已经走到叶子\n",
    "            return node.leaf_class\n",
    "        ## 当前节点的工作\n",
    "        # 本样本最终要往哪个分支走\n",
    "        goto = None\n",
    "        # 根据当前节点的最佳特征及最佳切分点决定x是继续往左边走还是往右边走\n",
    "        feature_value = x[node.best_feature_i]\n",
    "        # 离散型/连续型特征处理不同\n",
    "        if is_linear:\n",
    "            if feature_value >= node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node#往右边\n",
    "        else:\n",
    "            if feature_value == node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node\n",
    "        return self._search_class(x, goto, is_linear)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cf2d2",
   "metadata": {},
   "source": [
    "**cart回归树**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec082749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"通过树结点的各属性记录生成的树结构\"\"\"\n",
    "    def __init__(self,\n",
    "                 best_feature_i=None, \n",
    "                 best_split_point=None,\n",
    "                 left_node=None, \n",
    "                 right_node=None,\n",
    "                 leaf_value = None,\n",
    "                 is_leaf=False,\n",
    "                 variance=None):\n",
    "        \"\"\"\n",
    "        每个当前结点Node都记录了当前的划分状况\n",
    "        :param left_child_node : 结点的左侧子结点\n",
    "        :param right_child_node : 结点的右侧子结点\n",
    "        :param best_feature_i : 当前结点的最佳划分特征\n",
    "        :param split_point : 当前结点的最佳特征对应的最佳分割点\n",
    "        :param leaf_value : 记录当前节点的预测值\n",
    "        :param is_leaf : 只有在is_leaf==True时，leaf_class才生效\n",
    "        :param variance : 当前节点的平方误差\n",
    "        \n",
    "        \"\"\"\n",
    "        self.best_feature_i = best_feature_i\n",
    "        self.best_split_point = best_split_point\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.leaf_value = leaf_value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.variance = variance\n",
    "        \n",
    "class CartRegressionTree():\n",
    "    \"\"\"使用cart算法构建决策树\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth = float(\"inf\"),min_sample_split=2, min_variance_decrease=None):\n",
    "        # 代表决策树的决策树根节点\n",
    "        self.root_node = None \n",
    "        # 预设的决策树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 预设的决策树叶子节点最小样本数\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # 预设的基尼系数增益的最小值（variance_gain太小时不划分）\n",
    "        self.min_variance_decrease = min_variance_decrease\n",
    "        \n",
    "    def fit(self, X,y,is_linear=False):\n",
    "        \"\"\"\n",
    "        决策树拟合\n",
    "        :param X : 训练数据集∈（m,n）\n",
    "        :param y : 训练标签∈（n,1）\n",
    "        :param is_linear : 特征是否为连续型\n",
    "        \n",
    "        \"\"\"\n",
    "        # 创建决策树根结点\n",
    "        self.root_node = Node()\n",
    "        # 默认根节点的深度为1\n",
    "        cur_depth = 1\n",
    "        # 根节点的初始化权重\n",
    "        # 样本的初始权重:都为1\n",
    "        weight = np.ones((len(X))) # 全局的weight:初始化为全1 \n",
    "        # 递归构建决策树\n",
    "        self._build_tree_recussive(X,y,np.arange(len(X)),weight,self.root_node, cur_depth, is_linear)\n",
    "    \n",
    "    def _build_tree_recussive(self, X,y, node_indices,weight,node:Node, cur_depth, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,递归建立决策树\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param node : 当前结点的状态记录\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = len(node_indices)\n",
    "        n_features = X.shape[1]\n",
    "        # 记录本节点的状态\n",
    "        node.variance = self._variance(y[node_indices], weight[node_indices])\n",
    "        node.leaf_value = self._leaf_value(y[node_indices])\n",
    "\n",
    "        ## 递归基\n",
    "        # 节点包含数据全为同一个值，此时无需划分\n",
    "        if len(np.unique(y[node_indices])) <= 1:\n",
    "            # 记录叶子结点所属的分类\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 没有更多特征(当前节点所含样本所有特征都只有一个取值)\n",
    "        if np.sum([len(np.unique(X[node_indices][:,i])) for i in range(n_features)]) == n_features:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制构建子树的深度\n",
    "        if cur_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制节点的最小样本量\n",
    "        if n_samples < self.min_sample_split:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "        ## 处理当前节点自身\n",
    "        # 找到最佳特征和特征的最佳分割点\n",
    "        best_feature_i,best_variance_gain, best_sets = self._get_best_split_feature(X, y, node_indices, weight, is_linear)\n",
    "        \n",
    "        # 基尼系数增益的最小值（gini_gain太小时不划分）\n",
    "        if self.min_variance_decrease is not None and  best_variance_gain < self.min_variance_decrease:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        # 基于最佳特征和最佳分割点分成左右子树left,right\n",
    "        left_indices = best_sets[\"left_indices\"]\n",
    "        right_indices = best_sets[\"right_indices\"]\n",
    "        left_weight = best_sets[\"left_weight\"]\n",
    "        right_weight = best_sets[\"right_weight\"]\n",
    "        # 记录本节点的状态\n",
    "        node.best_feature_i = best_feature_i\n",
    "        node.best_split_point = best_sets[\"best_split_point\"]\n",
    "        node.left_node = Node()\n",
    "        node.right_node = Node()\n",
    "        # --leaf_class和gini在递归基时记录\n",
    "\n",
    "        # 让buildtree()帮忙划分左右子树\n",
    "        self._build_tree_recussive(X,y,left_indices,left_weight, node.left_node, cur_depth+1, is_linear)\n",
    "        self._build_tree_recussive(X,y,right_indices,right_weight, node.right_node, cur_depth+1, is_linear)\n",
    "                                   \n",
    "    def _get_best_split_feature(self, X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_variance_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的基尼增益以及分割的左右子树\n",
    "            cur_variance_gain,cur_branch_sets = self._get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_variance_gain = lou * cur_variance_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"],best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_variance_gain, best_sets\n",
    "    \n",
    "    def _get_best_split_point(self, X,y,node_indices, weight, feature_i, is_linear=False):\n",
    "\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "        :param weight: 所有样本的权重\n",
    "        :param is_linear : 特征的类型\n",
    "        :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "        \"\"\"\n",
    "        ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "        # 当前特征i的所有候选分割点\n",
    "        split_points, split_func = self._create_split_points(X[node_indices], feature_i, is_linear)\n",
    "\n",
    "        ## 产生最佳切分点\n",
    "        # 初始化\n",
    "        best_variance_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "        best_sets = None# 存储最佳切分点切分的左右分支\n",
    "        # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "        for point in split_points:\n",
    "            # 使用每个候选分割点进行二分分割\n",
    "            left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "            right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "            ## 分好左右分支后计算划分后的variance_gain\n",
    "            # 左右分支的权重计算不再使用频数\n",
    "            w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "\n",
    "            # 未划分前的平方误差和:使用该属性上无缺失的样本来计算\n",
    "            cur_variance = self._variance(y[node_indices],weight[node_indices])\n",
    "            # 未划分前-划分后(左右)==variance_gain\n",
    "            cur_variance_gain = cur_variance - (w_left*self._variance(y[left_indices],weight[left_indices]) + w_right*self._variance(y[right_indices], weight[right_indices]))\n",
    "\n",
    "            # 选择最佳的split point\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                # 划分时传给左右子集的weight、indices均不同weigh\n",
    "                # weight在之后再重置\n",
    "                best_sets = {\n",
    "                    \"best_split_point\":point,\n",
    "                    \"left_indices\": left_indices,\n",
    "                    \"right_indices\": right_indices,\n",
    "                }\n",
    "        return best_variance_gain, best_sets\n",
    "    \n",
    "    def _create_split_points(self, X, feature_i, is_linear=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func\n",
    "    \n",
    "     \n",
    "    \n",
    "    def _variance(self, y, weight):\n",
    "        \"\"\"\n",
    "        :param y : 当前节点样本的连续标签\n",
    "        :param weight : 当前节点样本的权重\n",
    "        :return square_variance : 平方误差和\n",
    "\n",
    "        \"\"\"\n",
    "        mean_y = np.mean(y, axis=0)\n",
    "        sum_square_variance = np.sum(np.square(y-mean_y)*weight)\n",
    "        return sum_square_variance\n",
    "\n",
    "    def _leaf_value(self, y):\n",
    "        return np.mean(y, axis=0)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def predict(self,X,is_linear=False):\n",
    "        \"\"\"\n",
    "        :param X: 待预测的m个样本\n",
    "\n",
    "        \"\"\"\n",
    "        # 每一个样本都通过二叉搜索决策树树查找所属类别,决策树由其根节点作为代表\n",
    "        y_pred = [self._search_yhat(x, self.root_node, is_linear) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _search_yhat(self, x, node:Node, is_linear=False):\n",
    "        \"\"\"\n",
    "        : param x: 待预测所属分类的样本\n",
    "        : param node : 当前所在节点\n",
    "\n",
    "        \"\"\"\n",
    "        # 递归基\n",
    "        if node.is_leaf:# 已经走到叶子\n",
    "            return node.leaf_value\n",
    "        ## 当前节点的工作\n",
    "        # 本样本最终要往哪个分支走\n",
    "        goto = None\n",
    "        # 根据当前节点的最佳特征及最佳切分点决定x是继续往左边走还是往右边走\n",
    "        feature_value = x[node.best_feature_i]\n",
    "        # 离散型/连续型特征处理不同\n",
    "        if is_linear:\n",
    "            if feature_value >= node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node#往右边\n",
    "        else:\n",
    "            if feature_value == node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node\n",
    "        return self._search_yhat(x, goto, is_linear)\n",
    "        \n",
    "    def prune(self, alpha=0):\n",
    "        \"\"\"对决策树进行后剪枝\"\"\"\n",
    "        return self._pruning_node(self.root_node, alpha)\n",
    "        \n",
    "    def _pruning_node(self, node,alpha):\n",
    "        \"\"\"\n",
    "        :param node : 当前处理的节点\n",
    "        :param alpha: loss的参数，alpha≥0\n",
    "\n",
    "        \"\"\"\n",
    "        ## 递归基:当前节点是叶子节点则直接返回\n",
    "        if node.is_leaf:\n",
    "            return \n",
    "        ## 让递归函数帮忙处理左右子树\n",
    "        self._pruning_node(node.left_node, alpha)\n",
    "        self._pruning_node(node.right_node, alpha)\n",
    "\n",
    "        ## 处理当前节点\n",
    "        # 剪枝后\n",
    "        post_loss = node.variance + alpha * 1 \n",
    "        # 剪枝前\n",
    "        pre_loss = node.left_node.variance + node.right_node.variance + alpha * 2\n",
    "        # 比较剪枝前的loss与剪枝后的loss\n",
    "        if post_loss < pre_loss: # 剪枝后loss更小则剪枝（收回左右结点）\n",
    "            node.left_node = None\n",
    "            node.right_node = None\n",
    "            node.best_feature_i = None\n",
    "            node.best_split_point = None\n",
    "            node.is_leaf = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf0c80",
   "metadata": {},
   "source": [
    "**汇合成cart决策树**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a93273",
   "metadata": {},
   "source": [
    "先定义共同的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c60d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"通过树结点的各属性记录生成的树结构\"\"\"\n",
    "    def __init__(self,\n",
    "                 best_feature_i=None, \n",
    "                 best_split_point=None,\n",
    "                 left_node=None, \n",
    "                 right_node=None,\n",
    "                 leaf_value = None,\n",
    "                 is_leaf=False,\n",
    "                 metric=None):\n",
    "        \"\"\"\n",
    "        每个当前结点Node都记录了当前的划分状况\n",
    "        :param left_child_node : 结点的左侧子结点\n",
    "        :param right_child_node : 结点的右侧子结点\n",
    "        :param best_feature_i : 当前结点的最佳划分特征\n",
    "        :param split_point : 当前结点的最佳特征对应的最佳分割点\n",
    "        :param leaf_value : 记录当前节点所属的类别(分类树)or均值(回归树)\n",
    "        :param is_leaf : 只有在is_leaf==True时，leaf_class才生效\n",
    "        :param metric : 当前节点的gini_index(分类树)or平方误差variance(回归树)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.best_feature_i = best_feature_i\n",
    "        self.best_split_point = best_split_point\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.leaf_value = leaf_value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.metric = metric\n",
    "        \n",
    "class CartDecisionTree():\n",
    "    \"\"\"使用cart算法构建决策树框架\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth = float(\"inf\"),min_sample_split=2, min_metric_decrease=None):\n",
    "        # 代表决策树的决策树根节点\n",
    "        self.root_node = None \n",
    "        ## 以下是预剪枝相关参数\n",
    "        # 预设的决策树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 预设的决策树叶子节点最小样本数\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # 预设的基尼系数增益的最小值（variance_gain太小时不划分）\n",
    "        self.min_metric_decrease = min_metric_decrease\n",
    "        ## 以下是分类/回归树的特定参数\n",
    "        # 计算gini/variance的函数\n",
    "        self.compute_metric = None\n",
    "        # 计算叶子节点类别/预测均值的函数\n",
    "        self.compute_leaf_value = None\n",
    "        \n",
    "    def fit(self, X,y,is_linear):\n",
    "        \"\"\"\n",
    "        决策树拟合\n",
    "        :param X : 训练数据集∈（m,n）\n",
    "        :param y : 训练标签∈（n,1）\n",
    "        :param is_linear : 特征是否为连续型\n",
    "        \n",
    "        \"\"\"\n",
    "        # 创建决策树根结点\n",
    "        self.root_node = Node()\n",
    "        # 默认根节点的深度为1\n",
    "        cur_depth = 1\n",
    "        # 根节点的初始化权重\n",
    "        # 样本的初始权重:都为1\n",
    "        weight = np.ones((len(X))) # 全局的weight:初始化为全1 \n",
    "        # 递归构建决策树\n",
    "        self._build_tree_recussive(X,y,np.arange(len(X)),weight,self.root_node, cur_depth, is_linear)\n",
    "    \n",
    "    def _build_tree_recussive(self, X,y, node_indices,weight,node:Node, cur_depth, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,递归建立决策树\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param node : 当前结点的状态记录\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = len(node_indices)\n",
    "        n_features = X.shape[1]\n",
    "        # 记录本节点的状态\n",
    "        node.metric = self.compute_metric(y[node_indices], weight[node_indices])\n",
    "        node.leaf_value = self.compute_leaf_value(y[node_indices], weight[node_indices])\n",
    "\n",
    "        ## 递归基\n",
    "        # 节点包含数据全为同一个值，此时无需划分\n",
    "        if len(np.unique(y[node_indices])) <= 1:\n",
    "            # 记录叶子结点所属的分类\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 没有更多特征(当前节点所含样本所有特征都只有一个取值)\n",
    "        if np.sum([len(np.unique(X[node_indices][:,i])) for i in range(n_features)]) == n_features:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制构建子树的深度\n",
    "        if cur_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制节点的最小样本量\n",
    "        if n_samples < self.min_sample_split:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "        ## 处理当前节点自身\n",
    "        # 找到最佳特征和特征的最佳分割点\n",
    "        best_feature_i,best_gain, best_sets = self._get_best_split_feature(X, y, node_indices, weight, is_linear)\n",
    "        \n",
    "        # 增益的最小值（gini_gain太小时不划分）\n",
    "        if self.min_metric_decrease is not None and  best_gain < self.min_metric_decrease:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        # 基于最佳特征和最佳分割点分成左右子树left,right\n",
    "        left_indices = best_sets[\"left_indices\"]\n",
    "        right_indices = best_sets[\"right_indices\"]\n",
    "        left_weight = best_sets[\"left_weight\"]\n",
    "        right_weight = best_sets[\"right_weight\"]\n",
    "        # 记录本节点的状态\n",
    "        node.best_feature_i = best_feature_i\n",
    "        node.best_split_point = best_sets[\"best_split_point\"]\n",
    "        node.left_node = Node()\n",
    "        node.right_node = Node()\n",
    "        # --leaf_class和gini在递归基时记录\n",
    "\n",
    "        # 让buildtree()帮忙划分左右子树\n",
    "        self._build_tree_recussive(X,y,left_indices,left_weight, node.left_node, cur_depth+1, is_linear)\n",
    "        self._build_tree_recussive(X,y,right_indices,right_weight, node.right_node, cur_depth+1, is_linear)\n",
    "                                   \n",
    "    def _get_best_split_feature(self, X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的增益以及分割的左右子树\n",
    "            cur_gain,cur_branch_sets = self._get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_gain = lou * cur_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_gain >= best_gain:\n",
    "                best_gain = cur_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"],best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_gain, best_sets\n",
    "    \n",
    "    def _get_best_split_point(self, X,y,node_indices, weight, feature_i, is_linear):\n",
    "\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "        :param weight: 所有样本的权重\n",
    "        :param is_linear : 特征的类型\n",
    "        :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "        \"\"\"\n",
    "        ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "        # 当前特征i的所有候选分割点\n",
    "        split_points, split_func = self._create_split_points(X[node_indices], feature_i, is_linear)\n",
    "\n",
    "        ## 产生最佳切分点\n",
    "        # 初始化\n",
    "        best_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "        best_sets = None# 存储最佳切分点切分的左右分支\n",
    "        # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "        for point in split_points:\n",
    "            # 使用每个候选分割点进行二分分割\n",
    "            left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "            right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "            ## 分好左右分支后计算划分后的variance_gain\n",
    "            # 左右分支的权重计算不再使用频数\n",
    "            w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "\n",
    "            # 未划分前的平方误差和/gini:使用该属性上无缺失的样本来计算\n",
    "            cur_metric = self.compute_metric(y[node_indices],weight[node_indices])\n",
    "            # 未划分前-划分后(左右)==variance_gain\n",
    "            cur_gain = cur_metric - (w_left*self.compute_metric(y[left_indices],weight[left_indices]) + w_right*self.compute_metric(y[right_indices], weight[right_indices]))\n",
    "\n",
    "            # 选择最佳的split point\n",
    "            if cur_gain >= best_gain:\n",
    "                best_gain = cur_gain\n",
    "                # 划分时传给左右子集的weight、indices均不同weigh\n",
    "                # weight在之后再重置\n",
    "                best_sets = {\n",
    "                    \"best_split_point\":point,\n",
    "                    \"left_indices\": left_indices,\n",
    "                    \"right_indices\": right_indices,\n",
    "                }\n",
    "        return best_gain, best_sets\n",
    "    \n",
    "    def _create_split_points(self, X, feature_i, is_linear):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func\n",
    "    \n",
    "    def predict(self,X,is_linear=False):\n",
    "        \"\"\"\n",
    "        :param X: 待预测的m个样本\n",
    "\n",
    "        \"\"\"\n",
    "        # 每一个样本都通过二叉搜索决策树树查找所属类别,决策树由其根节点作为代表\n",
    "        y_pred = [self._search_yhat(x, self.root_node, is_linear) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _search_yhat(self, x, node:Node, is_linear):\n",
    "        \"\"\"\n",
    "        : param x: 待预测所属分类的样本\n",
    "        : param node : 当前所在节点\n",
    "\n",
    "        \"\"\"\n",
    "        # 递归基\n",
    "        if node.is_leaf:# 已经走到叶子\n",
    "            return node.leaf_value\n",
    "        ## 当前节点的工作\n",
    "        # 本样本最终要往哪个分支走\n",
    "        goto = None\n",
    "        # 根据当前节点的最佳特征及最佳切分点决定x是继续往左边走还是往右边走\n",
    "        feature_value = x[node.best_feature_i]\n",
    "        # 离散型/连续型特征处理不同\n",
    "        if is_linear:\n",
    "            if feature_value >= node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node#往右边\n",
    "        else:\n",
    "            if feature_value == node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node\n",
    "        return self._search_yhat(x, goto, is_linear)\n",
    "        \n",
    "    def prune(self, alpha=0):\n",
    "        \"\"\"对决策树进行后剪枝\"\"\"\n",
    "        return self._pruning_node(self.root_node, alpha)\n",
    "        \n",
    "    def _pruning_node(self, node,alpha):\n",
    "        \"\"\"\n",
    "        :param node : 当前处理的节点\n",
    "        :param alpha: loss的参数，alpha≥0\n",
    "\n",
    "        \"\"\"\n",
    "        ## 递归基:当前节点是叶子节点则直接返回\n",
    "        if node.is_leaf:\n",
    "            return \n",
    "        ## 让递归函数帮忙处理左右子树\n",
    "        self._pruning_node(node.left_node, alpha)\n",
    "        self._pruning_node(node.right_node, alpha)\n",
    "\n",
    "        ## 处理当前节点\n",
    "        # 剪枝后\n",
    "        post_loss = node.metric + alpha * 1 \n",
    "        # 剪枝前\n",
    "        pre_loss = node.left_node.metric + node.right_node.metric + alpha * 2\n",
    "        # 比较剪枝前的loss与剪枝后的loss\n",
    "        if post_loss < pre_loss: # 剪枝后loss更小则剪枝（收回左右结点）\n",
    "            node.left_node = None\n",
    "            node.right_node = None\n",
    "            node.best_feature_i = None\n",
    "            node.best_split_point = None\n",
    "            node.is_leaf = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f61202",
   "metadata": {},
   "source": [
    "**定义分类树的特殊部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481e8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartClassificationTree(CartDecisionTree):\n",
    "    \"\"\"定义Cart分类树\"\"\"\n",
    "    def _gini(self,y, weight):\n",
    "        \n",
    "        \"\"\"\n",
    "        计算当前样本集合的gini指数\n",
    "        :param y : 当前集合对应的标签\n",
    "        :param weight : 当前样本集合对应的权重\n",
    "\n",
    "        \"\"\"\n",
    "        # 找出y中包含的所有类别\n",
    "        unique_y = np.unique(y)\n",
    "        weight_sum = np.sum(weight)\n",
    "        # 1.计算p_k:计算无缺失样本中第k类所占的比例\n",
    "        p_k = [np.sum(weight[y==k])/ weight_sum for k in unique_y ]\n",
    "        # 2.计算gini\n",
    "        gini = np.sum([p * (1-p) for p in p_k])\n",
    "        return gini\n",
    "    \n",
    "    def _majority_vote(self, y,weight):\n",
    "        \"\"\"\n",
    "        根据多数原则定叶子节点所属的分类\n",
    "        :param y : 当前集合对应的标签\n",
    "        :param weight : 前集合对应的权重\n",
    "        :return most_common_class叶子节点所属的分类\n",
    "        \"\"\"\n",
    "        most_common_class = None\n",
    "        max_distribution = 0\n",
    "        for k in np.unique(y): \n",
    "            distribution = np.sum(weight[y==k])\n",
    "            if distribution >= max_distribution:\n",
    "                max_distribution = distribution\n",
    "                most_common_class = k\n",
    "        return most_common_class \n",
    "    \n",
    "    def fit(self, X,y, is_linear=False):\n",
    "        \"\"\"分类树拟合\"\"\"\n",
    "        # 为之前的CartDecision进行初始化\n",
    "        self.compute_metric = self._gini\n",
    "        self.compute_leaf_value = self._majority_vote\n",
    "        super(CartClassificationTree, self).fit(X,y,is_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82fde01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartRegressionTree(CartDecisionTree):\n",
    "    \"\"\"定义cart回归树\"\"\"\n",
    "    def _variance(self, y, weight):\n",
    "        \"\"\"\n",
    "        :param y : 当前节点样本的连续标签\n",
    "        :param weight : 当前节点样本的权重\n",
    "        :return square_variance : 平方误差和\n",
    "\n",
    "        \"\"\"\n",
    "        mean_y = np.mean(y, axis=0)\n",
    "        sum_square_variance = np.sum(np.square(y-mean_y)*weight)\n",
    "        return sum_square_variance\n",
    "\n",
    "    def _leaf_value(self, y,weight):\n",
    "        return np.mean(y, axis=0)\n",
    "    \n",
    "    def fit(self, X, y, is_linear=False):\n",
    "        \"\"\"回归树拟合\"\"\"\n",
    "        # 为之前的CartDecision进行初始化\n",
    "        self.compute_metric = self._variance\n",
    "        self.compute_leaf_value = self._leaf_value\n",
    "        super(CartRegressionTree, self).fit(X,y,is_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b719b",
   "metadata": {},
   "source": [
    "分类树测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2026ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5569fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return np.sum(y_pred == y_true) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da926c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "       [np.nan, 1, 1, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, np.nan],\n",
    "       [1, 1, np.nan, 1, 1, 1],\n",
    "       [2, 1, 2, 1, 1, 1],\n",
    "       [np.nan, 1, 1, 1, 1, 1],\n",
    "       [2, 2, 1, 1, np.nan, 2],\n",
    "       [1, 2, 1, 2, 2, 2],\n",
    "       [1, 2, 1, np.nan, 2, 1],\n",
    "       [1, np.nan, 2, 2, 2, 1],\n",
    "       [2, 3, 3, np.nan, 3, 2],\n",
    "       [3, 3, 3, 3, 3, np.nan],\n",
    "       [3, 1, np.nan, 3, 3, 2],\n",
    "       [np.nan, 2, 1, 2, 1, 1],\n",
    "       [3, 2, 2, 2, 1, 1],\n",
    "       [1, 2, 1, 1, np.nan, 2],\n",
    "       [3, 1, 1, 3, 3, 1],\n",
    "       [2, np.nan, 2, 2, 2, 1]])\n",
    "y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "model = CartClassificationTree()\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "accuracy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00163f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X,y = data.data, data.target\n",
    "X_train,test_X,y_train,test_y = train_test_split(X,y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ece91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不输入参数——不进行预剪枝\n",
    "model = CartClassificationTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "\n",
    "accuracy(y_pred, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01083a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入参数——预剪枝\n",
    "model = CartClassificationTree(max_depth=3, min_sample_split=3, min_metric_decrease=0.1)\n",
    "model.fit(X_train,y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "accuracy(y_pred,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc52310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28888888888888886"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prune(0.2)\n",
    "y_pred = model.predict(test_X)\n",
    "accuracy(y_pred,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27b38f",
   "metadata": {},
   "source": [
    "回归树测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de8c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\yangdaopy\\Pycharm\\pythondownload\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, test_X, y_train, test_y = train_test_split(X,y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ec5142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.842105263157896"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "mean_squared_error(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a52da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.207565599995371"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree(max_depth = 6,min_sample_split=5, min_metric_decrease=3)\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "mean_squared_error(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e62dfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.842105263157896\n",
      "14.682051892006804\n"
     ]
    }
   ],
   "source": [
    "# 创建决策树不使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "print(mean_squared_error(test_y, y_pred))\n",
    "# 创建决策树后进行后剪枝\n",
    "model.prune(0.999)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "print(mean_squared_error(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfb65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
