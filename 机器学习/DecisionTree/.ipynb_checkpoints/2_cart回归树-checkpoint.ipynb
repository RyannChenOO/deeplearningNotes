{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ba911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAACSCAYAAAAXZoCqAAAgAElEQVR4nO3de1gUZd8H8O8uy/msIuIJMVREUIzUPCD5kFnqK2qW5iEtwTLLMk9RlorWPpqKj71qL5GampamaI+SlnhWRFwTQUTkoLa6CqicYYHdff9YZ5plZ2GBkV3o97murnaZmZ17cZjv3Ie5R6TRaDQghBBCiGDEpi4AIYQQ0tJQuBJCCCECo3AlhBBCBEbhSgghhAiMwpUQQggRGIUrIYQQIjAKV0IIIURgFK6EEEKIwChcCSGEEIFRuBJCCCECo3AlhBBCBEbhSgghhAiMwpUQQggRGIUrIYQQIjAKV0IIIURgFK6EEEKIwChcCSGEEIFRuBJCCCECo3AlhBBCBEbhSgghhAiMwpUQQggRGIUrIYQQIjAKV0IIIURgFK6EEEKIwChcCSGEEIFRuBJCCCECo3Ctg6asHKiqMnUxCCGENCPNNlw1FRVQ3boNdX7+U91P2UopCoaEoDL2V0CtbtRnVR79AxXbdvB+jqasHI9790Px1LdR+Ud8o/ZDCCHEtCSmLkCdNBqoH+RClX4DqrR0VKddhyotHaqsbG1ISSRw3B4Dy6DB9f7oiu+2QHX9BqwmjIPlwAGASKS3jvr+A6jvKVAydz7sKypg/cbrDf4qVUePQbl3P6qOHoPjji2AxIJdJrKzhcjCAlWnzkCVmQWr4SEN3g8hhBDTMutw1RQWovCl/4H6nkJvmcjWFhY9usHypRch6RvQoM8XOTtDuXc/lHv3w27pZ7AJm1GjABqoc24BAMTubWHZyMBT3boNAKi+kgxNeRlEjo46y8UeHlDnP4TV2P9p1H4IIYSYllmHq8jZGU77dqPy6DGoc3NR8W0MxO5t4fDNOkj6BQLixrVqW73yEkrnf6J9w6lFMlTpN6DKzAYAOGxcD3Gb1g3elzo/H9WyPwEANu/N0gtWANCUlwEALJ8LROX+g6i6dBma3DxYT5wAy+H/avC+CSGENC2zDlcAEHfsAJuZ0wEAlXv2Qa24j8rfjkIyoB80FRUoj/oG1q+OhUX3bvX+bJGjI8Qe7aBW3Iekl6/ecuXuvQAAq/GhkAzo16jvUblnP6BWQ9zOHTZhb2l/qFZDffceVDduQpVxk62hF781S2fb6ivJcHkhCLC0bFQZCCGENA2zDVfVjQxYeD8DWHD6JV1dgfyHEHfqBADQlJSiYlM0Kr6Ngd0nC2AzO7ze+xG3bQu14j5Ezk46P1f/JUfFjl0QWVvDbvF8aErLUC27jKo/jqMqMQmSwADYzp4FcedOde5DU1SE8k3RAADbRR9DZGsLTUkpCl8eA/XtOzrrimxtIenbB+KuXWDRvRskvf1h0asnBSshhDQjZhuuxTNmQWRnB4dvN8Cim7f2h9ZWAABxWzcAQOUvsdqfq9WoTk0DNBreQUm1srEGoA01rrJVa4HqasDaCkWjX4U6L09nuep6OpQ/7YXj9i2wDBpU6y7KN0VDU1gIC18fWI8P1e7PwR7OcbGoPHoMEIshspSgZM48WE+fArvPFtfvOxBCCDErZnsrjs2MqVBl3ETJu3O1oQlAZPHkWsDaCqrsHJSv0faDOvzf/8JhY1T9gxWAyFobrmVLV6JgwFBUbIpGdUoqKg8egiSwL+w+j4BV6CjYR62G86nf4ZJwEpbBQdqNq1UQPQl8Q9QPcqH8/gftd5o5Q7cm7uQE69fGw/rVsUC1CgAg6ekD5Y7dKBz2MoonvYmqhMR6fydCCCGmJdJoniSXmdEUF6NgSAg0BYVolXUNkEhQFPoaqi9fgeMP36H8m80Qt3OH/ZfLIWrlWr/PfvgIyv0HUXXiJKoSLmprqAAsuneD/frVKP/qa6hz8+D48w7eQUyVvx9DyczZgMQCrbLSah1YVbroMyh37wEASPo9B6f9u9llqlu3tctKy1B18RJU19N5P8Nx9w+wHFJ77ZgQQoj5MNtmYZGjIxx3fA9Nbj4geVLMJzVXtfwurEYMhySgN6rOJUCdlw9NXh7UuXnQlJZC1Lo1bN56ExbeXXk/uyoxCRU/7IRafpcNRodvv4HVyBFQXb8BcZfOsFv2GYonz4BFxw6wfHEYrCdOYGudFh06aMtoZVVrsFadPA3l7j2w8POF5v4DqG5k6K5QXY0Kpi/2g9mwmPseLLp4QtyhPcr/87+oeFLjtXiG/3sQQggxT2ZRc9U8fATVPQU0BYXQFDyGpqAQ6oICaB4XQlNQ8OR9IVTXrmmnIzSCyNoazsd/MzzgSKMBqqpQPGsOquJPwvX6nxA5OPy9XK1G8dS3UHXmPADAJvwt2H3xKQBAlZmNwmEjIHJygus1Ge/Hq/Mfomj4aKiLiuC0bzdKZs2BpqAQrhlXdcpQ0G8I1A9yYS9dAfXdu1BlZkPyfH+IbG1QungJxB07wCXhpFHfmRBCiHkwfc1Vo0Hxm2GovppicBWxmxvEnp0Aa2ugrBxW48bAoquXtuZoZQVoNNBUlAPlSu3/oZ30QdS6lvtSRSLttgxLS2gKClF5+DdYT5kEiMVw3P49yr/ZDOWO3Wy/r85HWBkYwavRoHRhBNRFRXDYGAVJQG/t/ixq1HJFIog7doD6QS7Klq2EyNERFr16QtymNTTlT76HRzvD34EQQohZMn24ikRw3PE9ylb+G6o7f8Gi2zOweOYZbfOoZyeIO3ZgR/IWvTYF1RcuwmbyREie7y9oMSr/G4fyf6+BfdTqv38okcB23gewnfeB9n11NTQqFaDWDj4ydHuM8udfoJbfg9O+3dpgfbIt06zNxUwm4ZxwAiJbO+0tSL4+qPxvnHa5i7MwX5AQQkiTMX24AhC1coX9ulX12KD+o4J5VaugKSgEAJQuXgKHdav05yjWaFB96TIqtv8ITf5DOGzewG7D9gXXIHk2AM6vjdcZGaxRqXhngRI52AMASqaHo/p6OlCtgtXoV9gRySJbu8Z+S0IIIU3MLMLVFKpTUlG68FOorl2HuL0HHL7bCElvf3a5prAQVSdOo+L7bai+chVWI4bD4ftvIbKzhfrhIwCAyEDNlXe2KEO37TwJYMmQQbCeOQMWnTtB0jcAyp+1s0OJbG0a+U0JIYQ0teYZrg2tuVarUHUhEZWxv0L5SyygVkMS2BcOMZvZW25Umdko/XABqlOvsY+Gs5nzDuwWfcyODFYr7ms/z9jgU6mgKSmByL6t/rInn2n5wlCos3Kg3HcAqjt/sfsW2VHNlRBCmpvmGa4NnLBfnZ+Pktlz2WZdy8EDtbVR+78DzMLLEyK31oBaDZGzMxyiVutNml+deFG7bvv2xu03Nw9QqXSaiRmiJ9+leOI07Wd27wYLL0/t4C3A+AAnhBBiNppluNacB9hY4nbucNy1DaXzFkPcoT0c/u8biGxqhJeFBRy//xbK2F9h+Xx/iDt20FmsKS5mb88Rd+5o1H7Vd+9py11zXwB7oeD820GIvbuy6zD3uIpsbaF5XKCdfrGqGhbdvWmeYUIIMXPNM1xdXRq8rcTfD86//1fbtGyoednCAtYTxrFvlXv3o+r4SVgOGoiqhERoiou1nzV4oFH7ZJqRa4arpqwcmgolAKDq1Blojh2H5tFjqB8+giotDQBQvvY/KF/7H3YbcTt3OO7cAose3Y37woQQQppcswxXsXMjb0+pZ7Oy9dj/QcX321B56Le/P8K9rdFTEjJhDBtrVGz+Dsqf90L9IA+akhJ2nbJ/r9G+EIkgdm8Lsbs7LIMGQWRnD4hF0JSUQlOmfd5r1elzFK6EEGLGml24iju0b/pmUUtLOKxbjcJR457cr2oB+3+v1HuSjiHMKGSLbt6wmjAWqhsZqJb9CVFPH0h69YSFrw8svLpA3KE9xO3cqdmXEEKauWYVrmJHR4j7+Ne94lNg4esDu6WfQrl1B+y++BSWIS8Yv22vnrAKHQ2rF/8FsZsb7Nd//RRLSgghxNTMYm5hY5V++gWshr2gN3q3STXkmbGEEEL+UZpVuGpKStkZjQghhBBz1azClRBCCGkOGjYbAyGEEEIMonAlhBBCBEbhSgghhAiMwpUQQggRGIUrIYQQIjAKV0IIIURgFK6EEEKIwChcCSGEEIFRuBJCCCECo3AlhBBCBEbhSgghhAiMwpUQQggRGIUrIYQQIjAKV0IIIURgFK6EEEKIwChcCRFIfHw84uPjTV0MXsePH0dGRoapi0GasejoaJw+fdrUxWg2KFwJEUBsbCwiIiKgUChMXRRen3zyCf7zn/+YuhikmVuwYAH+/PNPUxejWaBwJaSRjhw5AqlUikmTJmHq1KmmLo5BSUlJpi4CacbCwsLQp08fvPPOO8jKyjJ1ccyeSKPRaExdCEKaq6SkJCxYsABDhw7FihUrTF0cg/r37w8AuHjxoolL8s8ze/bsBm3XunVrjBw5EoMGDRK4RA134cIFzJ07F05OTti9ezfc3NxMXSSzJTF1AZ6Wb7/9FmKxGLNmzTJ1UUgLlZWVhS+//BLl5eWYMmWKqYtTpzFjxpi6CP84MpkMMpmswdv//vvvcHd3R3BwMMLCwuDi4iJg6erv+eefx7Rp07Bjxw4sXrwYmzdvhrW1tUnLZK5aZLh+88032LFjB/ueApY8DT/++CPu3buHcePGwcfHx9TFIWYoMDAQFy9eRHl5OW7duoUzZ84gJiaGXb5kyRL4+/vDy8tLZ7vU1FQkJydj3759kMvl2LNnDy5fvozFixejT58+Tf01dMycORNJSUlITU3F+vXrsXjxYpOWx1y1uD7XdevWYceOHfD19UXXrl0RExOD6OhoUxeLtDAXL17EoUOHAADjxo0zcWmIubO1tUXPnj2RmJio8/N+/frpBSsA+Pn5YcqUKdi/fz+6du0KAMjMzER4eDgyMzObpMyG2NnZYebMmQCAP/74A7m5uSYtj7lqUeEqlUrx008/YcCAAZBKpZBKpRSw5Kn4+eefAQChoaFUayVGKSsrQ0pKCvvez88PHh4edW73r3/9S+f90aNHBS9bfQUHB6Ndu3YoKirC77//burimKUWE65Lly5FbGwsRo4cidWrV8PDwwNeXl6QSqXo06cPBSwRzJUrV3DmzBkAwMsvv2zi0hBjKZVK3Lx5k+0Hrfnf03b9+nWd9/7+/kZt16tXL533zLFnaswgOQpXfi2izzU+Ph6XLl1CeHg4wsPDdZZ5eXlh48aN2LRpE44fP47hw4fzNsMQYiym5jBgwAAEBgaauDSET0lJCZKTk3HlyhX2/3U5ePCgUTXJhkpLS9N537t3b6O2y87O1nl/7949wcrUGAMHDsSvv/6K9PR0HD9+XK+G/U/XIsI1JCQEHh4e8PX15V1uZWWFjz76CDk5ORSspFHu37/PhmuPHj1MXBrC59y5c1i3bh3++usvANpBRaGhoXXeNvI0gxXQDlLiMrbmWvOeUk9PT8HK1BghISGws7NDWVkZEhISKFxraBHhCsBgsHJRsJLGunLlCkpKSgAYf3I0B66urqYuQpPYuXMnNmzYgL59++K1117DsGHD4O7ubupiAdAN14CAALRt27bObYqLi/WarIcOHSp42Rpq4MCBiI+P17twIC0oXPnk5ubiwYMHcHd3N+pAJsIrLy+HXC5Ht27dGvwZqamp6Ny5M5ycnGpd7/Hjx/UKkbKyMjx69AiPHj1Cx44d0apVqzq34dYinn32WaP3xZDL5SgqKoKXlxdsbW3rvX1Ty8jIQOfOnWFjY8O7XC6Xw8bGBm3atGnikukbPXo0cnNz8f777+PNN980dXF03L17F3l5eez7vn37GrVdXFwcHjx4wL739vY2q9HpL774IuLj45GVlQW5XI6OHTuaukhmo8WFa3R0NDIyMnDjxg2dg9LHxwfjxo0zqwOzpcrKykJCQgJkMhnOnTsHAHBzc0NoaChmzZqFCxcu4OrVq+jduzeef/553s/IycnB/v37ERcXh+LiYgDaeU1ff/11vXWjo6Nx8OBB5OXlISwsrM77mi9duoRDhw4hLi5O5+ddu3aFn58fOwMNH6b/y87ODo6OjrX/Ijjy8/MhlUrZwSju7u6YNm0a7/dh5OTkYOvWrbC1tUVERITR++JTn4nYHjx4gLVr10Imk7G/+2HDhuGll15CSEgIAGDr1q04cOAAO5dyYGAg+vbta7J7ymfPno3c3Fx89tlnCA0NNUkZalNz6kljwlWhUODAgQM62yxfvtwsLmQY3Cbq1NRUCleOFhWuUqkUsbGxsLS0xCuvvIJevXrBzc0NaWlpKCgogFQqhaWlJUaPHm3qorZYycnJiIyMxF9//YWgoCAsWLAAdnZ2OHjwIHvz/M6dO1FRUQGAfzq+goICLF26FJWVlVi4cCHu3LmDmJgYrFmzBkFBQXp9Y9yb8mNiYgye4PPz8/H111/jxIkTALS3OPTo0QPdu3fHnTt3kJiYiF9//RUWFhYGw4ypudZVi+YqKyvDRx99BACYN28eiouL2e9jZ2dn8HhcsWIF29zWVBNVJCYmYtWqVVCpVJg+fTq6du2K8+fP4+jRozhx4gSkUilKS0uxefNmjB8/HsOHD0dCQgKSk5MRExNT6wXT0/LLL79AJpNh3rx5ZhmsgG64Ojo6GvU7WrlyJXu8BQcHY+nSpXBwcHhqZWwIbj92UlISjZ7naBHhmp+fj5kzZ0KhUMDDwwMrVqzQGYmnVCrx1VdfAQBOnDjRrMK1KW4REGrEa0JCApYuXYqCggK9GuTo0aOxatUqnSA0tN8VK1aguroaP/30EwDoPMbtv//9r87nlpeXw9XVFY8fP661bFevXoVUKkVWVhZatWqFjz76SOdEMHjwYLzwwgsIDQ3F0aNHMX/+fFhZWel8RlVVFTtSsz611i1btkCtVmPXrl0AgLNnz7LLUlJSeI/HO3fu6PRjNTZYjam55uTk4LPPPkNAQADmz5/PXsQMGTIEeXl5OHXqFHvRMWHCBCxatAhyuRyZmZlITk5my92U4fr48WPs2rULwcHBeOONN5psv/XFDdfu3bvrLMvPz0d+fj5KS0sBaB8EkZSUhHv37pl9i5uzszMsLS1RVVVl8sktzE2zD9ecnBxMnDgRgLY2cfDgQb11NmzYwDZvNacDQCaTNXjS7/owpim1LqdOncInn3wClUoFR0dH3s9bvHgx9u3bx7738/PTWyc2NhZnzpzB9u3b2Z9xH3F1//59nfVtbW0xY8YMREVFGSzblStXdMqzbNky3gDw8PBAly5dcOvWLdy+fVuvnzg/P599bWzN9dKlS9i+fTvWrFnD/ox7a0VRURHvdtzbNry9vY3aV2Nt3boV7du3x5IlS/TmsO3cuTP72tXVFZMnTwag/fc6f/48u8zOzq5JyspISkqCXC7H8uXLm3S/9XHz5k0UFBSw72UyGXuPKB8nJycMGjQI7777brOoCbZp0wYKhULn74O0gHB999132dfLli3TWy6Xy3WesckEcXMQGBiIzZs3N8l+GqOiogJbtmyBSqUCoA1rPmVlZTrv+UbbxsbGYtCgQTo1Ne7Ju127dnrbvPHGG7hx4wbi4uJ4a5Q//PAD+3rMmDEGa1apqam4desW3NzceAdgcU8extZcY2NjERAQoDPCk+mHBgyPYOe2WNSs6XDl5OTAxsamzttI6qq5JiQk4MiRI1i3bh3v5PDcC4Lg4GC2b61mi4Exx1JGRkat36k+bt26hbZt25r1yO2a/a2G/j4yMzNx8uRJFBUV4ciRI7X2x5sTJlwfPnxo6qKYlWYdrp9//jn7x+3n54chQ4bordOxY0eMHj0aMpkM4eHhzapJGBCuyfZp2rJlCzv7zMCBAw02z9WcoabmTfSZmZlIT09HZGQk+7Nz585BLpez70eOHMn72X369EFcXJxeyOzbt48NMxcXF0ybNo13+5ycHKxcuRIADPbbcUd7GlNzvXv3Lv744w+d/tvExESdmrih78OtuRq6zSwyMhKHDh2Cr68vtm3bVmd5anP48GF06dKF928IANtHDkBn4vjw8HC0a9cOf/75JyZMmFBryCsUCoSFhSEvLw+rV6/GCy+80KgyA9p/t4aM2m5K3HB1c3PDhAkTDI5Mj46OZrtO3n77bcF+T08T0++qVqvrPWK/JWu24Xro0CGdOTZr6+f54osvmqJI/0gymUznxF7bKEhubczT01OvhuTt7a1XU+dO9RYUFGRwNCJzUuc2NVdUVOjUWl966SV4enqiqKgIRUVFKCgoQFZWFi5evIgLFy6gurq61iZy7pW5MTXXDh06YPPmzejZsyf7M+5UcYMGDeL9Psw0fYzBgwfzfj7z+zSm2biummtdA6YyMjLY19yLIg8PD6O7FGQyGXuBItRFY0ZGBkaMGFHn1KYikYh9rdFoIBKJ2N8Js6zm7G5C4YZr7969a73la9asWTh9+jT7+z527JjZh2vr1q3Z1/n5+RSuTzTbcK05nZk5PVD4n+SPP/7QeV9bLYJbYzP02CzuSbekpASnT59m39d28zzzZI7g4GD2Zzk5OTp9tHv27MGePXv0tvX398eMGTPw4osv1lrzYgacADD6HlXu96msrNQZzGTomOUOZOrSpQs6dOjAu97ChQtx7949QY792sIuMTGRHbPQsWNHdOrUqUH7GD16NIqLi9G3b996DQirTX5+vs4gucZ49dVXjbrXuT6Sk5NRWVnJvjdmspvAwEA2XNPT0wUtz9Ngb2/Pvi4vLzdhScxLsw1Xbh+Qt7c37+AY8vRxB4i1bt261vlSuTVXY+ZVTUxMZPs5HRwcEBQUZHDd+/fvw8bGRickcnJydNYJCwuDr6+vTjD27NnT6EE43Cv0hpDJZDp9lIZCkRvAhppp61ompGvXrrGvjZ38wBChR/Q+88wz6Nu3Lz744ANBP1coNR8xZ0y4cltI7ty5I3iZhMYtrzndg2tqzTZcuc1Ujf2DJw3HDdfnnnvO4HrcR20BhmuuXNwwfu6552qtVRw+fBhDhw7VuX2G27QKoNEjohsbrtwm7gEDBhhs4r569Sr72lCTcFPihqupH9Rdk7e3t94IcnPC7Tu3tbXVe8INnwsXLjzNIgmOO9CvsX8jLUmzDNdbt27pNLUIOWH0jRs34OnpqTfd26NHj5CTk4NOnTrxTqWYkZEBLy8vWFpaClYWAE1yK86HH37YoPsob926pTMCuF+/fgbX5TZ1uri4GDX5OHeb2voV4+PjoVAoMH36dJ2fc5vUhOgHauyJgxtS3H5YLoVCoXMhwjcKVqlU4t69e00yV7ZarRYkXG/evNmoKTAN8fT0xJEjR1BRUWFwikZT4h7Dffr0qbOMMpnM4O1ZdW3H1wpTWFiIzMxMtGrVivd4kcvlsLe3b9TfB7d1ydrausGf09I0y3C1sLDQed++fftGf+bly5d17pWbO3cuQkJCsHfvXsTGxqKwsJAdjDFo0CCsX78eALBo0SLIZDJ0794dKSkpCA4OxkcffVTnEziM1RSTSHD7EuuDufWGUVvNlduMz20SPnLkCH755RfefjNuc1NtV/xnzpyBm5ubXp+sj48P+/sTYvJ2bpOXUqms9/aPHj1iXw8YMIB3He5YAh8fH72JLHbs2IGYmBh4enrC1tYWs2bNEmxwkEwmQ1VVFfz9/dl+tGvXrrHlru2iaNmyZVAqlZBKpTo/z8vLw8KFC1FUVASlUslOgSmUTp064caNG9i+fbvJpl40hJlHmhEQEFDnNtzbBgHUOSNTTk4OlEolIiMjUVRUhDfeeAOzZs3CuXPnsGXLFty8eRNdunRBeno6QkJC2H+f9evX48SJE7C3t0dmZiaGDh2K8ePHN6j/nvk7pVqrrmYZrp06ddKZlcfYR0VlZmbi9u3b7PyojIqKCqxfvx6zZ8+Gm5sbZs+ejVWrVuHMmTMoKyvD+++/zx50I0aMYO+7HDVqFKZNm4b58+fD3d0du3fvRlRUFKqrq7Fq1SpBvivf9IDmwtPTExKJBNXV1QBqv8i5ceMG+5pbG2PmGebDvfXFUI3p0qVLiIuLQ1hYmF5/D/dKnSmjMaKjo3lP1NyTB/f2IGNxw7Vr166863AHcNUcR3DkyBEcOHAAUqkU1tbWmD17NmJiYgQJ19mzZ7MXItOmTWP7MBMSEth1DPUXyuVyxMXF4cMPP9Rb9tVXX8HPzw8LFixgyytkCA4dOhRDhgzBzp07ERwcbFaPAWxIV0jNZ7U6OzvXuv7y5csRERGB77//HiNHjmQn0fnhhx+wdOlSvPTSSwCASZMmIT4+HmfOnMHevXvh7++PVatWwcfHBykpKZg5cyays7Px448/1uuBEmq1mj2uqb9Vl9jUBWgo7h+6MSPqjh49irlz52LDhg16y7Zv3w5ra2ud2VAKCgpw9+5dLFu2TOdqjjkp9u/fHzNmzMCkSZPYWhFzY7w5B6KQJBKJTkhww4MrNjZW59+ICY2qqipcunTJ4K0G3IsmZrRqTV999RXatWuH8ePH6y3r3r0720xvbL9cREQEYmJisH//ft7lTJA1JFyrqqrqXIc7k0/NJuFdu3Zh7NixGDRoEBuEdU37aIzExESdFhLuCZ47uMpQH/HGjRvh5uamN0VfbGwsCgoKsGDBAjx+/Ji9z1noWdK++OILqNVqSKVSs5oliJkSkmFMC1vNQXh8E3r0798fkZGRuHDhAh48eAAvLy822PLy8nD8+HFER0ezwQr83XU2f/58tG/fHuHh4WxXEHOcyeVyvf3Xhdu6JNTEIC1Fsw3XV199lX197NixWtf99ttv8fnnn8POzg5LlizRW/7nn3+yt3AwJ003NzcsWbLE4EjSsLAwvPbaazo/Y7Zt6ingTInbFMv3TMedO3fqNRUyAXX27Fnk5uYaDFduU27NB0YDwJo1ayCXy/Huu+/yXjX7+PiwM3KVlJTozFFc09mzZzF//nzEx8cjLCyMN6yBvwfPNWQUJ3e0M7eZnBEREaETctzbmn799Vfcvn2bnQSDGdBnqHm5PmrWlkaNGgVAG+bciyLuRBIMqVSK+PooxwMAAAr5SURBVPh4fPzxx3rH/bZt2zBlyhQA2n5xpn+eb5atxnBxccGKFSuQlpaG5cuX16uV4mniXmR37drVqBa2mrXGmn20J0+eBKD9HV69ehUhISGwsrLCkSNH2HWkUqleNwpzG1xQUBA++eQTnWXcC8X6PJAC0A3XZ555pl7btnTNslkY0N6GEBQUxM5D27ZtW3a6sMzMTGRkZCAzMxOZmZm4cOEChg8fjvfee4/3nkHudGTMwTt48GDe/iXm5Mc3iIq5IufOw9rSjR8/nn3c244dO1BVVYWQkBAkJydj586dOHXqFD744AOcPn2avZKXyWSwsrLCqlWrEBISYnCWojlz5uD999+HUqnEli1b4ObmBk9PTygUCkRHR7NBaGh7QNscdvnyZaSlpWHdunUQi8Xo06cPVCoV0tLS2P8SExPh4uKCTz/9FGPHjjX4eUytW6lUIicnp16DisaPH8+OGN60aRMmT56MwYMH4/bt29i2bZtO+Ht5eelcXPj7++Prr78GoJ2PmOmbFeIe13HjxiEmJgZ5eXm4ePEiysrKkJycjE2bNrG3zuzevRupqalISUmBv78/0tPTcezYMcTGxmLRokV6XS2AtkbJvZACtDNSPY0nuwwbNgxLlizBypUrsXDhQoSEhCAkJKTJnpkrl8vx5Zdfsu9LS0t1Qis7O1tncOKoUaN4Z4vr378/Dh06xLsPhUKBqKgo9OjRA2+++SauXbvGtiYwF3seHh68xyRz3uK7kOW2JNT3kXHcLh1jBin+kzTbcAWAtWvX4uTJk9iwYQPWrFmjMzk6IzAwsM5nPHL7rJjaF1//CHOAuri48I5eZa5Um+r+Q3PQpk0bREZGIioqCsnJyTpNYZ06dcLatWsRFBSEIUOGICIiQu8ks3DhQoOf3adPH6xcuRLfffcdUlNTdaYu9PT0RHR0dJ2DRNq2bYuYmBhERUVh7969WLx4sd46jo6OmDhxIsaMGVPniFbuKN+UlJR6hevgwYMxZ84c7N27FykpKTrTInp4eCAwMJA9xmpO7O7l5cXu6/z58ygqKkKPHj2MrrkaalZnhIaGIiYmRme/r7zyCubNmwdA21z922+/YebMmTrbhYeHY8KECbyfyW1CZ8YpPM3pR8eMGYPevXtj9erViIyMxNdff40RI0ZgxIgR6Ny5s2CDDPkkJSXVOfiQu3zOnDm867z88svsIxaZbRYtWgQHBwf8/vvv6NixIyIjI/Xu6WbOPXz979z98v29MLf+NORCjTuJDIWrrmYdroD2SmzAgAHIyclBdnY28vLy4OrqCldXV3Tu3LleJ7+cnBwUFhYC4D8ImYOUr28hNTWV7a+obbKDligwMBDR0dFISkrCw4cPoVQq0atXL3h7e7NNhV5eXtiyZQsyMzNx7do1FBcXY+zYsXXOiBMcHIxnn30Wly5dQn5+PiorK+Hj41OvyR8kEgkWLlyIqVOnIjs7G9nZ2ZBIJGjVqhVcXFzq1bTq6uoKf39/pKSk4OrVqxgzZozR2wLA9OnTMX36dNy8eRNpaWl48OABAG2t9vPPP2fXq20ieuaEJuRxNmvWLPTt2xf379+HQqHAwIEDdcqwfPlyzJgxA9nZ2ZDL5XB3d0fr1q1rvf2KwQzSGjhwYK0jyoXQpUsXrF+/Hj/++CMOHz6MAwcOsA8cl0gkcHJygpOTE5ydnSGR6J7+GvOQDGMfC6dQKHDv3r1aJ71h/i2OHj2K69evIyEhAXZ2dnj55ZcxZcoUvXOaXC5na5B8M6Qx5y1fX1/e2bWYYK5vuN6/f5+9mB4xYgRv//A/WbMPV0DbT+Hr62vU7Ce1YQZc+Pn58TYfM/0WfOHKXJl37tyZbRaWyWTNYuJ9IdjZ2elMPWhond69exs1OxOXo6Mjhg0b1pjiAdDWDj08PBo9McPo0aORkpLCTgvYkKn8unXrplNLlslk7EnQzc3N4DSSZ8+eZZuWma4JoY6zuoKSW3uuD+bh9EytWC6Xw8LCwuhR/vVlZWWFt956C2+99RZOnz6Nq1evori4GEVFRSguLta7hawpMcdgXfr162fUhQugvY2QwXccMOctvgrD+fPnIZfLYWtry16sGXs8cWutzeHReE2tRYSrUJirP0MzPjEnP74mY6ZphTlAMzMzsWjRIvz88880RL2FGTduHDsCOi4urs7HGMbHx2PDhg0oKSnB+PHjeZsEDx8+zL4ODQ01eMwwwTp06FC2ayIyMhIbN26sd39ZU5DJZGzthgnXvXv3QqFQYPXq1U99/0OHDq11TuqWgDlvBQQE8AY3M3iO77zFVApCQkLYbaOiojBv3rw6A5aZ2jEgIMAsZhIzN812tPDTwJwE+MKVCVZLS0u9WsWdO3fYvlqmvzUuLg6DBw+mYG2hmCZAbijyKS4uRkREBBQKBYqLi3lHVCcmJrKDWNzc3AyOVAb+fsIKU5OPi4uDs7OzWQYr8PffjZ+fH1tTT0pK0hntTxqHOab4zluPHz/G48ePYWNjw7ucG66ANlhdXV3rDNa//vqLbU5ubo/xbCoUrk+UlZUhKysL9vb2tfa3BgYG6jUDuru7s0Pme/bsyU5sMHny5KdfcGISzCPa0tPTERsba3C9u3fv6rwfPny43jrcWyM+/vjjWi/ImJG2o0aNQkZGBtavX29UX5+pMLeEMH9T69evR2BgoCC3EBHteYupmfKFJ7MsICCAt0+UmQEsICAACoUCv/32m8HBVly//PILAG3LQH3HHfxTULg+wfS3BgUF8d4qwPRb8A1EsLa2xtSpUwEAM2bMwM6dO/Hpp582aL5e0nxMmjQJAGoNV+6FWEhIiE6tNC8vD1OmTGGnn5RKpby3tHBNmTIFTk5OePvttzF16lRMnjzZrMN18ODB8PX1xc6dO9G/f39cvXoV8+fPN3WxWgzmNho/Pz/eQXDclgM+TL+9vb09IiMjsWTJkjrPW2fPnsXu3bsBQGcEP9El0tT1FOV/kNo68pk5PGs78GQyGdzd3c22iY4ILyoqCrt370ZERITBkIuIiEB8fDzCw8Ph7e0NJycnpKWl4ZtvvgGg7YucOHFivUb/pqam1usxi/3798eYMWN4J1FpCjdv3kS7du0Ee44r+Vtt5y2FQoHCwsI6z1vGDoirrKzEO++8g2vXruH999/Hm2++2aAy/xNQuBLSSO+88w7S09OxZs0agyM8d+3ahTNnzrA1CTs7O3h7e2Ps2LFN0mfFDCb6p0zNSZ6OzZs3Y+vWrToPASD8KFwJEUBwcDBcXV2xdu3aWqeBy8/PR3FxcZM8Lo6LwpU01qVLl/Dee+/Bx8cHmzZteiozbbUk1OdKiABOnTqFnj17Ys+ePbWu16ZNmyYPVkA7rR0NsCONsWbNGrz++uvYvn07BasRqOZKCCGECIxqroQQQojAKFwJIYQQgVG4EkIIIQKjcCWEEEIERuFKCCGECIzClRBCCBEYhSshhBAiMApXQgghRGAUroQQQojAKFwJIYQQgVG4EkIIIQKjcCWEEEIERuFKCCGECIzClRBCCBEYhSshhBAiMApXQgghRGAUroQQQojAKFwJIYQQgVG4EkIIIQKjcCWEEEIERuFKCCGECIzClRBCCBEYhSshhBAiMApXQgghRGAUroQQQojAKFwJIYQQgVG4EkIIIQKjcCWEEEIERuFKCCGECIzClRBCCBEYhSshhBAiMApXQgghRGAUroQQQojAKFwJIYQQgVG4EkIIIQKjcCWEEEIERuFKCCGECOz/AbuF7g9mBw1vAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "f696ae38",
   "metadata": {},
   "source": [
    "与分类树相比，回归树需要注意以下问题：\n",
    "\n",
    "**回归树的划分：**虽然与分类树不同，分类树的标签是离散变量，而回归树的标签是连续变量。但是回归树与分类树的划分都是为了分类出与y相近的同一类人\n",
    "\n",
    "    怎样对输入空间进行划分？——分类树与回归树相比，仅仅是标签变了，输入空间（x）未变，所以切分点的产生、特征的处理不变。\n",
    "    叶子节点的计算：分类树属于多数原则选出叶子所属的分类作为预测值，回归树算出本叶子节点的平均值作为预测值\n",
    "    怎样判断此次划分的合理性？——分类树对离散型标签采用最大增益（gini/information entropy）的方法进行特征选择， 而回归树使用平方误差最小准则进行特征选择。\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a3017",
   "metadata": {},
   "source": [
    "具体的划分策略：\n",
    "\n",
    "对于所有的特征变量：\n",
    "\n",
    "寻找特征变量的最优切分点，这个切分点会把数据集划分为两个子集，我们的目的是寻找那个能使两个子集标签平方误差之和最小的切分点。找到特征变量的最优切分点之后，再遍历特征变量，同样根据平方误差之和最小的准则，找到最优特征变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前样本X,y,weight,node_indices\n",
    "\"\"\"\n",
    ":param X : 所有样本\n",
    ":param y : 所有样本对应的标签\n",
    ":param weight : 所有样本对应的权重\n",
    ":param node_indices : 当前样本对应的索引\n",
    ":param is_linear : 所有特征的属性是连续型还是离散型\n",
    "\n",
    "\"\"\"\n",
    "n_samples, n_features = len(node_indices), X.shape[1]\n",
    "\n",
    "## 寻找最佳特征\n",
    "# 初始化\n",
    "best_feature_variance_gain = -1\n",
    "best_feature = None\n",
    "best_feature_sets = None\n",
    "for feature_i in range(n_features):\n",
    "    \n",
    "    ## 找出未缺失的样本\n",
    "    nonan_indices = [i if ~np.isnan(X[i, feature_i]) for i in node_indices]\n",
    "    # 找出缺失样本\n",
    "    nan_indices = [i if np.isnan(X[i, feature_i]) for i in node_indices]\n",
    "    # 未划分前的平方距离:使用该属性上无缺失的样本来计算\n",
    "    cur_variance = variance(y[nonan_indices], weight[nonan_indices])\n",
    "    # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "    lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "    \n",
    "    ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "    # 当前特征i的所有候选分割点\n",
    "    split_points, split_func = create_split_points(X[nonan_indices], feature_i, is_linear=False)\n",
    "    \n",
    "    ## 产生最佳切分点\n",
    "    # 初始化\n",
    "    best_variance_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "    best_sets = None# 存储最佳切分点切分的左右分支\n",
    "    # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "    for point in split_points:\n",
    "        # 使用每个候选分割点进行二分分割\n",
    "        left_indices = [i for i in nonan_indices if split_func(X[i], point)]\n",
    "        right_indices = [i for i in nonan_indices if not split_func(X[i], point)]\n",
    "        # 分好左右分支后计算划分后的variance_gain\n",
    "        # 左右分支的权重计算不再使用频数\n",
    "        w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "        cur_variance_gain = cur_variance - (w_left*variance(y[left_indices],weight[left_indices]) + w_right*variance(y[right_indices], weight[right_indices]))\n",
    "        # 选择最佳的split point\n",
    "        if cur_variance_gain >= best_variance_gain:\n",
    "            best_variance_gain = cur_variance_gain\n",
    "            best_sets = {\n",
    "                \"best_split_point\":point,\n",
    "                \"left_indices\": left_indices,\n",
    "                \"right_indices\": right_indices,\n",
    "            }\n",
    "    \n",
    "    ## 基于上述方法计算所有特征的加权gini_gain,找到最佳特征\n",
    "    # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "    cur_feature_gini_gain = lou * best_gini_gain\n",
    "    \n",
    "    if cur_feature_variance_gain >= best_featur_variance_gain:\n",
    "    best_feature_variance_gain = cur_feature_variance_gain\n",
    "    best_feature = feature_i\n",
    "    best_feature_sets = best_sets\n",
    "    # 找到最佳特征\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f40baa",
   "metadata": {},
   "source": [
    "模块化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70c96c",
   "metadata": {},
   "source": [
    "计算平方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(y, weight):\n",
    "    \"\"\"\n",
    "    :param y : 当前节点样本的连续标签\n",
    "    :param weight : 当前节点样本的权重\n",
    "    :return square_variance : 平方误差和\n",
    "    \n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y, axis=0)\n",
    "    sum_square_variance = np.sum(np.square(y-mean_y)*weight)\n",
    "    return sum_square_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [14,30,18,11]\n",
    "weight = np.ones(len(y))\n",
    "variance(y, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236930fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [22,26]\n",
    "weight = np.ones(len(y))\n",
    "variance(y, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4f982",
   "metadata": {},
   "source": [
    "产生候选切分点-与之前相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_split_points(X, feature_i, is_linear=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,11,1).reshape(-1,1)\n",
    "y = np.array([5.56, 5.7, 5.91, 6.4, 6.8, 7.05, 8.9, 8.7, 9, 9.05])\n",
    "create_split_points(x, 0, is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贷款数据集\n",
    "X = np.array([[10, 15], [11, 40], [12, 20], [9, 30], [13, 10], [8, 25]])\n",
    "y = np.array([14, 30, 18, 26, 11, 22])\n",
    "create_split_points(X,0,is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb13794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split_point(X,y,node_indices, weight, feature_i, is_linear=False):\n",
    "\n",
    "    \"\"\"\n",
    "    对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "    :param X: 所有样本\n",
    "    :param y: 所有标签\n",
    "    :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "    :param weight: 所有样本的权重\n",
    "    :param is_linear : 特征的类型\n",
    "    :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "    \"\"\"\n",
    "    ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "    # 当前特征i的所有候选分割点\n",
    "    split_points, split_func = create_split_points(X[node_indices], feature_i, is_linear)\n",
    "    \n",
    "    ## 产生最佳切分点\n",
    "    # 初始化\n",
    "    best_variance_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "    best_sets = None# 存储最佳切分点切分的左右分支\n",
    "    # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "    for point in split_points:\n",
    "        # 使用每个候选分割点进行二分分割\n",
    "        left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "        right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "        ## 分好左右分支后计算划分后的variance_gain\n",
    "        # 左右分支的权重计算不再使用频数\n",
    "        w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "        \n",
    "        # 未划分前的平方误差和:使用该属性上无缺失的样本来计算\n",
    "        cur_variance = variance(y[node_indices],    weight[node_indices])\n",
    "        # 未划分前-划分后(左右)==variance_gain\n",
    "        cur_variance_gain = cur_variance - (w_left*variance(y[left_indices],weight[left_indices]) + w_right*variance(y[right_indices], weight[right_indices]))\n",
    "        \n",
    "        # 选择最佳的split point\n",
    "        if cur_variance_gain >= best_variance_gain:\n",
    "            best_variance_gain = cur_variance_gain\n",
    "            # 划分时传给左右子集的weight、indices均不同weigh\n",
    "            # weight在之后再重置\n",
    "            best_sets = {\n",
    "                \"best_split_point\":point,\n",
    "                \"left_indices\": left_indices,\n",
    "                \"right_indices\": right_indices,\n",
    "            }\n",
    "    return best_variance_gain, best_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3138388",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1,11,1).reshape(-1,1)\n",
    "y = np.array([5.56, 5.7, 5.91, 6.4, 6.8, 7.05, 8.9, 8.7, 9, 9.05])\n",
    "weight = np.ones(len(X))\n",
    "get_best_split_point(X,y,range(len(x)), weight, 0, is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43777f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贷款数据集\n",
    "X = np.array([[10, 15], [11, 40], [12, 20], [9, 30], [13, 10], [8, 25]])\n",
    "y = np.array([14, 30, 18, 26, 11, 22])\n",
    "weight = np.ones(len(X))\n",
    "get_best_split_point(X,y,range(len(X)), weight, 0, is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split_feature(X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_variance_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的基尼增益以及分割的左右子树\n",
    "            cur_variance_gain,cur_branch_sets = get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_variance_gain = lou * cur_variance_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"],best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_variance_gain, best_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1,11,1).reshape(-1,1)\n",
    "y = np.array([5.56, 5.7, 5.91, 6.4, 6.8, 7.05, 8.9, 8.7, 9, 9.05])\n",
    "weight = np.ones(len(X))\n",
    "get_best_split_feature(X,y,range(len(X)),weight, is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b623c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贷款数据集\n",
    "X = np.array([[10, 15], [11, 40], [12, 20], [9, 30], [13, 10], [8, 25]])\n",
    "y = np.array([14, 30, 18, 26, 11, 22])\n",
    "weight = np.ones(len(X))\n",
    "get_best_split_feature(X,y,range(len(X)), weight,is_linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b511ef1",
   "metadata": {},
   "source": [
    "叶节点的处理：不再按照多数原则，而是计算均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd83b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaf_value(y):\n",
    "    return np.mean(y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493111ed",
   "metadata": {},
   "source": [
    "修改后的代码(含预剪枝，未包含后剪枝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66fbdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"通过树结点的各属性记录生成的树结构\"\"\"\n",
    "    def __init__(self,\n",
    "                 best_feature_i=None, \n",
    "                 best_split_point=None,\n",
    "                 left_node=None, \n",
    "                 right_node=None,\n",
    "                 leaf_value = None,\n",
    "                 is_leaf=False,\n",
    "                 variance=None):\n",
    "        \"\"\"\n",
    "        每个当前结点Node都记录了当前的划分状况\n",
    "        :param left_child_node : 结点的左侧子结点\n",
    "        :param right_child_node : 结点的右侧子结点\n",
    "        :param best_feature_i : 当前结点的最佳划分特征\n",
    "        :param split_point : 当前结点的最佳特征对应的最佳分割点\n",
    "        :param leaf_class : 记录当前节点所属的类别\n",
    "        :param is_leaf : 只有在is_leaf==True时，leaf_class才生效\n",
    "        :param gini : 当前节点的gini_index\n",
    "        \n",
    "        \"\"\"\n",
    "        self.best_feature_i = best_feature_i\n",
    "        self.best_split_point = best_split_point\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.leaf_value = leaf_value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.variance = variance\n",
    "        \n",
    "class CartRegressionTree():\n",
    "    \"\"\"使用cart算法构建决策树\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth = float(\"inf\"),min_sample_split=2, min_variance_decrease=None):\n",
    "        # 代表决策树的决策树根节点\n",
    "        self.root_node = None \n",
    "        # 预设的决策树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 预设的决策树叶子节点最小样本数\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # 预设的基尼系数增益的最小值（variance_gain太小时不划分）\n",
    "        self.min_variance_decrease = min_variance_decrease\n",
    "        \n",
    "    def fit(self, X,y,is_linear=False):\n",
    "        \"\"\"\n",
    "        决策树拟合\n",
    "        :param X : 训练数据集∈（m,n）\n",
    "        :param y : 训练标签∈（n,1）\n",
    "        :param is_linear : 特征是否为连续型\n",
    "        \n",
    "        \"\"\"\n",
    "        # 创建决策树根结点\n",
    "        self.root_node = Node()\n",
    "        # 默认根节点的深度为1\n",
    "        cur_depth = 1\n",
    "        # 根节点的初始化权重\n",
    "        # 样本的初始权重:都为1\n",
    "        weight = np.ones((len(X))) # 全局的weight:初始化为全1 \n",
    "        # 递归构建决策树\n",
    "        self._build_tree_recussive(X,y,np.arange(len(X)),weight,self.root_node, cur_depth, is_linear)\n",
    "    \n",
    "    def _build_tree_recussive(self, X,y, node_indices,weight,node:Node, cur_depth, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,递归建立决策树\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param node : 当前结点的状态记录\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = len(node_indices)\n",
    "        n_features = X.shape[1]\n",
    "        # 记录本节点的状态\n",
    "        node.variance = self._variance(y[node_indices], weight[node_indices])\n",
    "        node.leaf_value = self._leaf_value(y[node_indices])\n",
    "\n",
    "        ## 递归基\n",
    "        # 节点包含数据全为同一个值，此时无需划分\n",
    "        if len(np.unique(y[node_indices])) <= 1:\n",
    "            # 记录叶子结点所属的分类\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 没有更多特征(当前节点所含样本所有特征都只有一个取值)\n",
    "        if np.sum([len(np.unique(X[node_indices][:,i])) for i in range(n_features)]) == n_features:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制构建子树的深度\n",
    "        if cur_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制节点的最小样本量\n",
    "        if n_samples < self.min_sample_split:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "        ## 处理当前节点自身\n",
    "        # 找到最佳特征和特征的最佳分割点\n",
    "        best_feature_i,best_variance_gain, best_sets = self._get_best_split_feature(X, y, node_indices, weight, is_linear)\n",
    "        \n",
    "        # 基尼系数增益的最小值（gini_gain太小时不划分）\n",
    "        if self.min_variance_decrease is not None and  best_variance_gain < self.min_variance_decrease:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        # 基于最佳特征和最佳分割点分成左右子树left,right\n",
    "        left_indices = best_sets[\"left_indices\"]\n",
    "        right_indices = best_sets[\"right_indices\"]\n",
    "        left_weight = best_sets[\"left_weight\"]\n",
    "        right_weight = best_sets[\"right_weight\"]\n",
    "        # 记录本节点的状态\n",
    "        node.best_feature_i = best_feature_i\n",
    "        node.best_split_point = best_sets[\"best_split_point\"]\n",
    "        node.left_node = Node()\n",
    "        node.right_node = Node()\n",
    "        # --leaf_class和gini在递归基时记录\n",
    "\n",
    "        # 让buildtree()帮忙划分左右子树\n",
    "        self._build_tree_recussive(X,y,left_indices,left_weight, node.left_node, cur_depth+1, is_linear)\n",
    "        self._build_tree_recussive(X,y,right_indices,right_weight, node.right_node, cur_depth+1, is_linear)\n",
    "                                   \n",
    "    def _get_best_split_feature(self, X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_variance_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的基尼增益以及分割的左右子树\n",
    "            cur_variance_gain,cur_branch_sets = self._get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_variance_gain = lou * cur_variance_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"],best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_variance_gain, best_sets\n",
    "    \n",
    "    def _get_best_split_point(self, X,y,node_indices, weight, feature_i, is_linear=False):\n",
    "\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "        :param weight: 所有样本的权重\n",
    "        :param is_linear : 特征的类型\n",
    "        :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "        \"\"\"\n",
    "        ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "        # 当前特征i的所有候选分割点\n",
    "        split_points, split_func = self._create_split_points(X[node_indices], feature_i, is_linear)\n",
    "\n",
    "        ## 产生最佳切分点\n",
    "        # 初始化\n",
    "        best_variance_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "        best_sets = None# 存储最佳切分点切分的左右分支\n",
    "        # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "        for point in split_points:\n",
    "            # 使用每个候选分割点进行二分分割\n",
    "            left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "            right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "            ## 分好左右分支后计算划分后的variance_gain\n",
    "            # 左右分支的权重计算不再使用频数\n",
    "            w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "\n",
    "            # 未划分前的平方误差和:使用该属性上无缺失的样本来计算\n",
    "            cur_variance = self._variance(y[node_indices],weight[node_indices])\n",
    "            # 未划分前-划分后(左右)==variance_gain\n",
    "            cur_variance_gain = cur_variance - (w_left*self._variance(y[left_indices],weight[left_indices]) + w_right*self._variance(y[right_indices], weight[right_indices]))\n",
    "\n",
    "            # 选择最佳的split point\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                # 划分时传给左右子集的weight、indices均不同weigh\n",
    "                # weight在之后再重置\n",
    "                best_sets = {\n",
    "                    \"best_split_point\":point,\n",
    "                    \"left_indices\": left_indices,\n",
    "                    \"right_indices\": right_indices,\n",
    "                }\n",
    "        return best_variance_gain, best_sets\n",
    "    \n",
    "    def _create_split_points(self, X, feature_i, is_linear=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func\n",
    "    \n",
    "     \n",
    "    \n",
    "    def _variance(self, y, weight):\n",
    "        \"\"\"\n",
    "        :param y : 当前节点样本的连续标签\n",
    "        :param weight : 当前节点样本的权重\n",
    "        :return square_variance : 平方误差和\n",
    "\n",
    "        \"\"\"\n",
    "        mean_y = np.mean(y, axis=0)\n",
    "        sum_square_variance = np.sum(np.square(y-mean_y)*weight)\n",
    "        return sum_square_variance\n",
    "\n",
    "    def _leaf_value(self, y):\n",
    "        return np.mean(y, axis=0)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def predict(self,X,is_linear=False):\n",
    "        \"\"\"\n",
    "        :param X: 待预测的m个样本\n",
    "\n",
    "        \"\"\"\n",
    "        # 每一个样本都通过二叉搜索决策树树查找所属类别,决策树由其根节点作为代表\n",
    "        y_pred = [self._search_yhat(x, self.root_node, is_linear) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _search_yhat(self, x, node:Node, is_linear=False):\n",
    "        \"\"\"\n",
    "        : param x: 待预测所属分类的样本\n",
    "        : param node : 当前所在节点\n",
    "\n",
    "        \"\"\"\n",
    "        # 递归基\n",
    "        if node.is_leaf:# 已经走到叶子\n",
    "            return node.leaf_value\n",
    "        ## 当前节点的工作\n",
    "        # 本样本最终要往哪个分支走\n",
    "        goto = None\n",
    "        # 根据当前节点的最佳特征及最佳切分点决定x是继续往左边走还是往右边走\n",
    "        feature_value = x[node.best_feature_i]\n",
    "        # 离散型/连续型特征处理不同\n",
    "        if is_linear:\n",
    "            if feature_value >= node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node#往右边\n",
    "        else:\n",
    "            if feature_value == node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node\n",
    "        return self._search_yhat(x, goto, is_linear)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44b7d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\yangdaopy\\Pycharm\\pythondownload\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, test_X, y_train, test_y = train_test_split(X,y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30938ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.42276315789474"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "mean_squared_error(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eceb914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.5, 18.9, 27.5, 6.3, 34.9, 17.5, 17.3, 13.1, 24.3, 48.3, 20.6, 36.1, 17.5, 24.3, 16.7, 24.6, 25.0, 13.5, 11.8, 20.0, 20.6, 12.0, 31.2, 23.6, 8.7, 19.6, 11.8, 20.0, 24.3, 17.0, 17.0, 22.9, 46.7, 16.1, 12.7, 17.1, 25.0, 17.9, 23.7, 48.3, 21.9, 17.9, 26.5, 44.8, 22.3, 19.1, 20.3, 36.2, 14.5, 22.5, 22.2, 29.6, 31.5, 21.8, 25.0, 37.9, 31.6, 16.6, 27.9, 41.7, 13.9, 22.7, 13.3, 24.8, 48.5, 29.6, 14.1, 30.1, 19.1, 19.7, 50.0, 31.5, 18.3, 18.2, 22.9, 37.0, 24.5, 44.8, 26.2, 17.9, 23.7, 22.8, 24.7, 19.3, 44.8, 5.0, 16.2, 10.5, 23.1, 26.4, 31.2, 16.5, 18.9, 21.0, 6.3, 23.7, 11.8, 20.5, 20.9, 22.7, 14.9, 6.3, 14.1, 16.3, 27.5, 50.0, 23.2, 31.1, 18.7, 17.6, 21.9, 22.5, 20.7, 20.0, 14.5, 16.3, 11.5, 18.4, 23.1, 13.5, 19.6, 23.6, 24.4, 18.0, 31.1, 16.2, 27.9, 15.3, 16.7, 28.7, 23.7, 25.0, 17.5, 19.5, 5.6, 24.2, 17.0, 29.6, 16.4, 17.3, 23.6, 18.9, 22.0, 16.2, 36.1, 26.4, 14.1, 14.6, 23.9, 20.5, 31.2, 16.1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38cc4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.6 21.7 23.   8.5 32.9 19.4  7.  15.1 21.2 50.  29.8 33.2 16.6 41.3\n",
      " 12.5 22.3 22.  13.4  9.5 10.9 11.9 17.2 30.8 23.9  7.5 23.  13.4 21.2\n",
      " 27.  17.4 21.5 24.1 35.2 20.2 19.1 16.  23.3 13.8 23.7 21.9 16.8  7.\n",
      " 22.1 42.8 21.2 19.6 21.4 23.6 13.5 20.7 20.2 50.  34.6 22.4 33.  36.2\n",
      " 33.1 14.  20.8 37.6 10.2 19.6  8.3 23.  50.  34.9 13.8 26.6 10.2 20.\n",
      " 38.7 31.7 19.6 23.4 22.2 33.1 26.6 50.  28.6 13.8 22.5 18.5 21.7 21.7\n",
      " 50.  10.2 20.3  5.  22.2 23.4 29.1  8.1 17.4 18.6  8.8 30.1 13.4 18.6\n",
      " 20.3 19.4 14.1  7.2 14.9  7.2 15.4 50.  20.1 28.  17.8 17.2 19.9 21.1\n",
      " 28.7 20.8 11.7 12.1 10.9 16.1 20.6 17.8 19.8 37.2 21.6 18.4 24.1 24.\n",
      " 35.1 23.8 11.7 26.7 23.5 27.5 20.  27.1 10.5 23.1 19.4 33.4 14.3 15.7\n",
      " 28.7 18.8 17.7 24.3 33.4 22.6 19.1 11.8 22.2 19.8 30.5 21. ]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c346ac08",
   "metadata": {},
   "source": [
    "**后剪枝**\n",
    "\n",
    "分类树对叶子处理的方式是使用多数原则预测分类，预测误差使用gini，而回归树对叶子的处理方式是求均值，预测误差使用平方误差，所以根据公式，只用将求gini改为求平方误差variance\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(self, alpha=0):\n",
    "        \"\"\"对决策树进行后剪枝\"\"\"\n",
    "        return self._pruning_node(self.root_node, alpha)\n",
    "        \n",
    "def _pruning_node(self, node,alpha):\n",
    "    \"\"\"\n",
    "    :param node : 当前处理的节点\n",
    "    :param alpha: loss的参数，alpha≥0\n",
    "\n",
    "    \"\"\"\n",
    "    ## 递归基:当前节点是叶子节点则直接返回\n",
    "    if node.is_leaf:\n",
    "        return \n",
    "    ## 让递归函数帮忙处理左右子树\n",
    "    self._pruning_node(node.left_node, alpha)\n",
    "    self._pruning_node(node.right_node, alpha)\n",
    "\n",
    "    ## 处理当前节点\n",
    "    # 剪枝后\n",
    "    post_loss = node.variance + alpha * 1 \n",
    "    # 剪枝前\n",
    "    pre_loss = node.left_node.variance + node.right_node.variance + alpha * 2\n",
    "    # 比较剪枝前的loss与剪枝后的loss\n",
    "    if post_loss < pre_loss: # 剪枝后loss更小则剪枝（收回左右结点）\n",
    "        node.left_node = None\n",
    "        node.right_node = None\n",
    "        node.best_feature_i = None\n",
    "        node.best_split_point = None\n",
    "        node.is_leaf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ab9e0",
   "metadata": {},
   "source": [
    "修改后的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c459cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"通过树结点的各属性记录生成的树结构\"\"\"\n",
    "    def __init__(self,\n",
    "                 best_feature_i=None, \n",
    "                 best_split_point=None,\n",
    "                 left_node=None, \n",
    "                 right_node=None,\n",
    "                 leaf_value = None,\n",
    "                 is_leaf=False,\n",
    "                 variance=None):\n",
    "        \"\"\"\n",
    "        每个当前结点Node都记录了当前的划分状况\n",
    "        :param left_child_node : 结点的左侧子结点\n",
    "        :param right_child_node : 结点的右侧子结点\n",
    "        :param best_feature_i : 当前结点的最佳划分特征\n",
    "        :param split_point : 当前结点的最佳特征对应的最佳分割点\n",
    "        :param leaf_class : 记录当前节点所属的类别\n",
    "        :param is_leaf : 只有在is_leaf==True时，leaf_class才生效\n",
    "        :param gini : 当前节点的gini_index\n",
    "        \n",
    "        \"\"\"\n",
    "        self.best_feature_i = best_feature_i\n",
    "        self.best_split_point = best_split_point\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.leaf_value = leaf_value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.variance = variance\n",
    "        \n",
    "class CartRegressionTree():\n",
    "    \"\"\"使用cart算法构建决策树\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth = float(\"inf\"),min_sample_split=2, min_variance_decrease=None):\n",
    "        # 代表决策树的决策树根节点\n",
    "        self.root_node = None \n",
    "        # 预设的决策树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 预设的决策树叶子节点最小样本数\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # 预设的基尼系数增益的最小值（variance_gain太小时不划分）\n",
    "        self.min_variance_decrease = min_variance_decrease\n",
    "        \n",
    "    def fit(self, X,y,is_linear=False):\n",
    "        \"\"\"\n",
    "        决策树拟合\n",
    "        :param X : 训练数据集∈（m,n）\n",
    "        :param y : 训练标签∈（n,1）\n",
    "        :param is_linear : 特征是否为连续型\n",
    "        \n",
    "        \"\"\"\n",
    "        # 创建决策树根结点\n",
    "        self.root_node = Node()\n",
    "        # 默认根节点的深度为1\n",
    "        cur_depth = 1\n",
    "        # 根节点的初始化权重\n",
    "        # 样本的初始权重:都为1\n",
    "        weight = np.ones((len(X))) # 全局的weight:初始化为全1 \n",
    "        # 递归构建决策树\n",
    "        self._build_tree_recussive(X,y,np.arange(len(X)),weight,self.root_node, cur_depth, is_linear)\n",
    "    \n",
    "    def _build_tree_recussive(self, X,y, node_indices,weight,node:Node, cur_depth, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,递归建立决策树\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param weight : 所有样本对应的权重\n",
    "        :param node : 当前结点的状态记录\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = len(node_indices)\n",
    "        n_features = X.shape[1]\n",
    "        # 记录本节点的状态\n",
    "        node.variance = self._variance(y[node_indices], weight[node_indices])\n",
    "        node.leaf_value = self._leaf_value(y[node_indices])\n",
    "\n",
    "        ## 递归基\n",
    "        # 节点包含数据全为同一个值，此时无需划分\n",
    "        if len(np.unique(y[node_indices])) <= 1:\n",
    "            # 记录叶子结点所属的分类\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 没有更多特征(当前节点所含样本所有特征都只有一个取值)\n",
    "        if np.sum([len(np.unique(X[node_indices][:,i])) for i in range(n_features)]) == n_features:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制构建子树的深度\n",
    "        if cur_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        # 限制节点的最小样本量\n",
    "        if n_samples < self.min_sample_split:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "        ## 处理当前节点自身\n",
    "        # 找到最佳特征和特征的最佳分割点\n",
    "        best_feature_i,best_variance_gain, best_sets = self._get_best_split_feature(X, y, node_indices, weight, is_linear)\n",
    "        \n",
    "        # 基尼系数增益的最小值（gini_gain太小时不划分）\n",
    "        if self.min_variance_decrease is not None and  best_variance_gain < self.min_variance_decrease:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        # 基于最佳特征和最佳分割点分成左右子树left,right\n",
    "        left_indices = best_sets[\"left_indices\"]\n",
    "        right_indices = best_sets[\"right_indices\"]\n",
    "        left_weight = best_sets[\"left_weight\"]\n",
    "        right_weight = best_sets[\"right_weight\"]\n",
    "        # 记录本节点的状态\n",
    "        node.best_feature_i = best_feature_i\n",
    "        node.best_split_point = best_sets[\"best_split_point\"]\n",
    "        node.left_node = Node()\n",
    "        node.right_node = Node()\n",
    "        # --leaf_class和gini在递归基时记录\n",
    "\n",
    "        # 让buildtree()帮忙划分左右子树\n",
    "        self._build_tree_recussive(X,y,left_indices,left_weight, node.left_node, cur_depth+1, is_linear)\n",
    "        self._build_tree_recussive(X,y,right_indices,right_weight, node.right_node, cur_depth+1, is_linear)\n",
    "                                   \n",
    "    def _get_best_split_feature(self, X,y,node_indices,weight, is_linear):\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices,找到最佳特征：求所有特征的gini_gain\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引\n",
    "        :param is_linear : 特征的类型\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 获取样本数和特征数\n",
    "        n_samples, n_features = len(node_indices), X.shape[1]\n",
    "        \n",
    "        # 初始化\n",
    "        best_variance_gain = -1\n",
    "        best_feature = None\n",
    "        best_sets = None\n",
    "        \n",
    "        # 依次求所有特征的gini_gain\n",
    "        for feature_i in range(n_features):\n",
    "            # 特征在所有样本中取值唯一时无需找split_point和参与特征划分\n",
    "            if len(np.unique(X[node_indices][:,feature_i])) == 1:\n",
    "                continue\n",
    "                \n",
    "            ## 找出未缺失的样本\n",
    "            nonan_indices = [i  for i in node_indices if ~np.isnan(X[i, feature_i])]\n",
    "            # 找出缺失样本\n",
    "            nan_indices = [i  for i in node_indices if np.isnan(X[i, feature_i])]\n",
    "            # 无缺失值样本所占的比例:对每一个样本赋予了权重后,利用权重计算无缺失样本所占的比例\n",
    "            lou = np.sum(weight[nonan_indices]) / np.sum(weight[node_indices])\n",
    "            \n",
    "            # 特征i的基尼增益以及分割的左右子树\n",
    "            cur_variance_gain,cur_branch_sets = self._get_best_split_point(X,y,nonan_indices, weight,feature_i, is_linear)\n",
    "            # 找到特征i的最佳gini_point后,使用权重计算最终的gini_gain\n",
    "            cur_variance_gain = lou * cur_variance_gain\n",
    "            \n",
    "            # 寻找最佳特征\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                best_feature = feature_i\n",
    "                best_sets = cur_branch_sets\n",
    "                # 修改权重\n",
    "                left_weight, right_weight = np.zeros_like(weight),np.zeros_like(weight)\n",
    "                left_indices, right_indices = best_sets[\"left_indices\"],best_sets[\"right_indices\"]\n",
    "                left_weight[left_indices], right_weight[right_indices] = weight[left_indices], weight[right_indices]\n",
    "                left_weight[nan_indices], right_weight[nan_indices] = np.sum(weight[left_indices]) / np.sum(weight[nonan_indices]),np.sum(weight[right_indices]) / np.sum(weight[nonan_indices])\n",
    "\n",
    "                # 将缺失样本按不同的比重放到左右两个子集中\n",
    "                left_indices.extend(nan_indices)\n",
    "                right_indices.extend(nan_indices)\n",
    "                best_sets[\"left_indices\"] = left_indices\n",
    "                best_sets[\"right_indices\"] = right_indices\n",
    "                best_sets[\"left_weight\"] = left_weight\n",
    "                best_sets[\"right_weight\"] = right_weight\n",
    "\n",
    "        # 找到了当前节点所用的最佳特征（也找到了该特征的最佳分割点）\n",
    "        \n",
    "        return best_feature,best_variance_gain, best_sets\n",
    "    \n",
    "    def _get_best_split_point(self, X,y,node_indices, weight, feature_i, is_linear=False):\n",
    "\n",
    "        \"\"\"\n",
    "        对于当前节点集合（X，y）-node_indices计算特征i的基尼增益\n",
    "        :param X: 所有样本\n",
    "        :param y: 所有标签\n",
    "        :param node_indices : 当前样本集合对应的索引(无缺失值)\n",
    "        :param weight: 所有样本的权重\n",
    "        :param is_linear : 特征的类型\n",
    "        :return : 返回特征i的基尼增益以及分割的左右子树\n",
    "\n",
    "        \"\"\"\n",
    "        ## 基于无缺失的样本来寻找特征的最佳切分点\n",
    "        # 当前特征i的所有候选分割点\n",
    "        split_points, split_func = self._create_split_points(X[node_indices], feature_i, is_linear)\n",
    "\n",
    "        ## 产生最佳切分点\n",
    "        # 初始化\n",
    "        best_variance_gain = -1# 存储本特征的最佳切分点对应的gini_gain\n",
    "        best_sets = None# 存储最佳切分点切分的左右分支\n",
    "        # 依次使用候选分割点对当前集合（X，y）进行二分分割\n",
    "        for point in split_points:\n",
    "            # 使用每个候选分割点进行二分分割\n",
    "            left_indices = [i for i in node_indices if split_func(X[i],point)]\n",
    "            right_indices = [i for i in node_indices if not split_func(X[i], point)]\n",
    "            ## 分好左右分支后计算划分后的variance_gain\n",
    "            # 左右分支的权重计算不再使用频数\n",
    "            w_left, w_right = np.sum(weight[left_indices])/np.sum(weight), np.sum(weight[right_indices])/np.sum(weight)\n",
    "\n",
    "            # 未划分前的平方误差和:使用该属性上无缺失的样本来计算\n",
    "            cur_variance = self._variance(y[node_indices],weight[node_indices])\n",
    "            # 未划分前-划分后(左右)==variance_gain\n",
    "            cur_variance_gain = cur_variance - (w_left*self._variance(y[left_indices],weight[left_indices]) + w_right*self._variance(y[right_indices], weight[right_indices]))\n",
    "\n",
    "            # 选择最佳的split point\n",
    "            if cur_variance_gain >= best_variance_gain:\n",
    "                best_variance_gain = cur_variance_gain\n",
    "                # 划分时传给左右子集的weight、indices均不同weigh\n",
    "                # weight在之后再重置\n",
    "                best_sets = {\n",
    "                    \"best_split_point\":point,\n",
    "                    \"left_indices\": left_indices,\n",
    "                    \"right_indices\": right_indices,\n",
    "                }\n",
    "        return best_variance_gain, best_sets\n",
    "    \n",
    "    def _create_split_points(self, X, feature_i, is_linear=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        根据特征i是连续型/离散型特征得到特征i的所有候选分割点,并返回对应的分割函数\n",
    "        :param X: 当前集合的样本（无缺失值）\n",
    "        :param feature_i : 给定的特征索引\n",
    "        :return 特征i的所有候选分割点、分割函数\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # 1、确定特征i的所有可能取值\n",
    "        feature_values = np.unique(X[:, feature_i])# 已排序的 \n",
    "\n",
    "        # 2、根据特征i是连续型/离散型特征对这些可能取值进行处理从而得到特征i的所有候选分割点；    \n",
    "        split_points = None\n",
    "        split_func = None\n",
    "\n",
    "        if is_linear:\n",
    "            split_points = (feature_values[1:] + feature_values[:-1]) / 2 # 特征是连续型特征则使用二分法找到所有的切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] >= split_point\n",
    "\n",
    "        else:\n",
    "            split_points = feature_values# 离散型特征直接使用特征的各个取值作为切分点\n",
    "            split_func = lambda x,split_point : x[feature_i] == split_point\n",
    "        return split_points, split_func\n",
    "    \n",
    "     \n",
    "    \n",
    "    def _variance(self, y, weight):\n",
    "        \"\"\"\n",
    "        :param y : 当前节点样本的连续标签\n",
    "        :param weight : 当前节点样本的权重\n",
    "        :return square_variance : 平方误差和\n",
    "\n",
    "        \"\"\"\n",
    "        mean_y = np.mean(y, axis=0)\n",
    "        sum_square_variance = np.sum(np.square(y-mean_y)*weight)\n",
    "        return sum_square_variance\n",
    "\n",
    "    def _leaf_value(self, y):\n",
    "        return np.mean(y, axis=0)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def predict(self,X,is_linear=False):\n",
    "        \"\"\"\n",
    "        :param X: 待预测的m个样本\n",
    "\n",
    "        \"\"\"\n",
    "        # 每一个样本都通过二叉搜索决策树树查找所属类别,决策树由其根节点作为代表\n",
    "        y_pred = [self._search_yhat(x, self.root_node, is_linear) for x in X]\n",
    "        return y_pred\n",
    "\n",
    "    def _search_yhat(self, x, node:Node, is_linear=False):\n",
    "        \"\"\"\n",
    "        : param x: 待预测所属分类的样本\n",
    "        : param node : 当前所在节点\n",
    "\n",
    "        \"\"\"\n",
    "        # 递归基\n",
    "        if node.is_leaf:# 已经走到叶子\n",
    "            return node.leaf_value\n",
    "        ## 当前节点的工作\n",
    "        # 本样本最终要往哪个分支走\n",
    "        goto = None\n",
    "        # 根据当前节点的最佳特征及最佳切分点决定x是继续往左边走还是往右边走\n",
    "        feature_value = x[node.best_feature_i]\n",
    "        # 离散型/连续型特征处理不同\n",
    "        if is_linear:\n",
    "            if feature_value >= node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node#往右边\n",
    "        else:\n",
    "            if feature_value == node.best_split_point:\n",
    "                goto = node.left_node# 往左边\n",
    "            else:\n",
    "                goto = node.right_node\n",
    "        return self._search_yhat(x, goto, is_linear)\n",
    "        \n",
    "    def prune(self, alpha=0):\n",
    "        \"\"\"对决策树进行后剪枝\"\"\"\n",
    "        return self._pruning_node(self.root_node, alpha)\n",
    "        \n",
    "    def _pruning_node(self, node,alpha):\n",
    "        \"\"\"\n",
    "        :param node : 当前处理的节点\n",
    "        :param alpha: loss的参数，alpha≥0\n",
    "\n",
    "        \"\"\"\n",
    "        ## 递归基:当前节点是叶子节点则直接返回\n",
    "        if node.is_leaf:\n",
    "            return \n",
    "        ## 让递归函数帮忙处理左右子树\n",
    "        self._pruning_node(node.left_node, alpha)\n",
    "        self._pruning_node(node.right_node, alpha)\n",
    "\n",
    "        ## 处理当前节点\n",
    "        # 剪枝后\n",
    "        post_loss = node.variance + alpha * 1 \n",
    "        # 剪枝前\n",
    "        pre_loss = node.left_node.variance + node.right_node.variance + alpha * 2\n",
    "        # 比较剪枝前的loss与剪枝后的loss\n",
    "        if post_loss < pre_loss: # 剪枝后loss更小则剪枝（收回左右结点）\n",
    "            node.left_node = None\n",
    "            node.right_node = None\n",
    "            node.best_feature_i = None\n",
    "            node.best_split_point = None\n",
    "            node.is_leaf = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caac85f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\yangdaopy\\Pycharm\\pythondownload\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, test_X, y_train, test_y = train_test_split(X,y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7106d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.462368421052634"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "mean_squared_error(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bce520b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.530833998360038"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree(max_depth = 6,min_sample_split=5, min_variance_decrease=3)\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "mean_squared_error(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9877487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.462368421052634\n",
      "16.13281746814954\n"
     ]
    }
   ],
   "source": [
    "# 创建决策树不使用参数预剪枝\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "model = CartRegressionTree()\n",
    "model.fit(X_train, y_train, is_linear=True)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "print(mean_squared_error(test_y, y_pred))\n",
    "# 创建决策树后进行后剪枝\n",
    "model.prune(0.999)\n",
    "y_pred = model.predict(test_X, is_linear=True)\n",
    "print(mean_squared_error(test_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
